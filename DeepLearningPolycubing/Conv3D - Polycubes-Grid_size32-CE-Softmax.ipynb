{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"nbagg\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from utility import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import torchvision.utils as v_utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "\n",
    "polycube_path = \"/Users/davidcleres/DeepShape/Polycubing-Automated/Generated-Cars/\"\n",
    "polycube_files = [f for f in listdir(polycube_path) if isfile(join(polycube_path, f))]\n",
    "\n",
    "voxelized_mesh_path = \"/Users/davidcleres/DeepShape/Polycubing-Automated/voxelizedMeshes/\"\n",
    "voxelized_mesh_files = [f for f in listdir(voxelized_mesh_path) if isfile(join(voxelized_mesh_path, f))]\n",
    "\n",
    "voxelizedFiles = []\n",
    "polycubedFiles = []\n",
    "\n",
    "for f in voxelized_mesh_files: \n",
    "    if f[-13:] == \"voxelized.txt\":\n",
    "        voxelizedFiles = np.hstack((voxelizedFiles, f))\n",
    "    \n",
    "for f in polycube_files:\n",
    "    if f[-14:] == \"finalCubes.txt\":\n",
    "        polycubedFiles = np.hstack((polycubedFiles, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the global parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size=32\n",
    "batch_size=15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the tensor to a text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "<class 'torch.IntTensor'> torch.Size([32, 32, 32])\n",
      "torch.Size([60, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "voxelized_train_input, polycube_target=loadData(grid_size, polycube_path, voxelized_mesh_path, voxelizedFiles, polycubedFiles, loadFromScratch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train torch.Size([45, 1, 32, 32, 32])\n",
      "validation torch.Size([15, 1, 32, 32, 32])\n",
      "train_target torch.Size([45, 32, 32, 32])\n",
      "validation_target torch.Size([15, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "preprocessed_input_train, preprocessed_input_validation, preprocessed_input_train_target, preprocessed_input_validation_target = preprocessing_train(voxelized_train_input, polycube_target,batch_size, False, False)\n",
    "\n",
    "preprocessed_input_train = torch.from_numpy(preprocessed_input_train)\n",
    "preprocessed_input_validation = torch.from_numpy(preprocessed_input_validation)\n",
    "preprocessed_input_train_target = torch.from_numpy(preprocessed_input_train_target)\n",
    "preprocessed_input_validation_target = torch.from_numpy(preprocessed_input_validation_target)\n",
    "\n",
    "Ntrain = len(preprocessed_input_train[:, 0,0,0,0]) \n",
    "Nvalidation = len(preprocessed_input_validation[:,0,0,0,0])\n",
    "\n",
    "train_input = Variable(preprocessed_input_train.view(Ntrain, 1, grid_size, grid_size, grid_size).float())\n",
    "validation_input = Variable(preprocessed_input_validation.view(Nvalidation, 1, grid_size, grid_size, grid_size).float())\n",
    "\n",
    "labels_train = preprocessed_input_train_target.float()\n",
    "labels_validation = preprocessed_input_validation_target.float()\n",
    "\n",
    "print('train', train_input.shape)\n",
    "print('validation', validation_input.shape)\n",
    "print('train_target', labels_train.shape)\n",
    "print('validation_target', labels_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Initiating U-Net------\n",
      "\n",
      "Epoch Number :  0\n",
      "\t Training accuracy:  51.440090603298614\n",
      "\t Validation accuracy  50.734659830729164\n",
      "\t Epoch Loss  2.0747570991516113\n",
      "Epoch Number :  1\n",
      "\t Training accuracy:  51.33531358506944\n",
      "\t Validation accuracy  50.640462239583336\n",
      "\t Epoch Loss  2.072218894958496\n",
      "Epoch Number :  2\n",
      "\t Training accuracy:  51.574367947048614\n",
      "\t Validation accuracy  51.379191080729164\n",
      "\t Epoch Loss  2.0717477798461914\n",
      "Epoch Number :  3\n",
      "\t Training accuracy:  55.70482042100694\n",
      "\t Validation accuracy  54.8370361328125\n",
      "\t Epoch Loss  2.0723958015441895\n",
      "Epoch Number :  4\n",
      "\t Training accuracy:  50.737508138020836\n",
      "\t Validation accuracy  49.539388020833336\n",
      "\t Epoch Loss  2.07296085357666\n",
      "Epoch Number :  5\n",
      "\t Training accuracy:  52.62953016493056\n",
      "\t Validation accuracy  49.846394856770836\n",
      "\t Epoch Loss  2.0723657608032227\n",
      "Epoch Number :  6\n",
      "\t Training accuracy:  52.965291341145836\n",
      "\t Validation accuracy  50.163167317708336\n",
      "\t Epoch Loss  2.0723092555999756\n",
      "Epoch Number :  7\n",
      "\t Training accuracy:  52.533840603298614\n",
      "\t Validation accuracy  49.56787109375\n",
      "\t Epoch Loss  2.0721168518066406\n",
      "Epoch Number :  8\n",
      "\t Training accuracy:  51.81647406684028\n",
      "\t Validation accuracy  48.605550130208336\n",
      "\t Epoch Loss  2.0718560218811035\n",
      "Epoch Number :  9\n",
      "\t Training accuracy:  51.1865234375\n",
      "\t Validation accuracy  48.297526041666664\n",
      "\t Epoch Loss  2.0716090202331543\n",
      "Epoch Number :  10\n",
      "\t Training accuracy:  50.973782009548614\n",
      "\t Validation accuracy  49.72412109375\n",
      "\t Epoch Loss  2.0713889598846436\n",
      "Epoch Number :  11\n",
      "\t Training accuracy:  51.3104248046875\n",
      "\t Validation accuracy  51.076456705729164\n",
      "\t Epoch Loss  2.071228265762329\n",
      "Epoch Number :  12\n",
      "\t Training accuracy:  51.44361707899306\n",
      "\t Validation accuracy  51.592203776041664\n",
      "\t Epoch Loss  2.0711798667907715\n",
      "Epoch Number :  13\n",
      "\t Training accuracy:  50.89857313368056\n",
      "\t Validation accuracy  51.035563151041664\n",
      "\t Epoch Loss  2.0710525512695312\n",
      "Epoch Number :  14\n",
      "\t Training accuracy:  50.507473415798614\n",
      "\t Validation accuracy  50.0469970703125\n",
      "\t Epoch Loss  2.0708975791931152\n",
      "Epoch Number :  15\n",
      "\t Training accuracy:  50.211520724826386\n",
      "\t Validation accuracy  49.017537434895836\n",
      "\t Epoch Loss  2.070779323577881\n",
      "Epoch Number :  16\n",
      "\t Training accuracy:  49.97545030381944\n",
      "\t Validation accuracy  48.55712890625\n",
      "\t Epoch Loss  2.0707130432128906\n",
      "Epoch Number :  17\n",
      "\t Training accuracy:  49.714016384548614\n",
      "\t Validation accuracy  48.441365559895836\n",
      "\t Epoch Loss  2.070666551589966\n",
      "Epoch Number :  18\n",
      "\t Training accuracy:  49.606051974826386\n",
      "\t Validation accuracy  48.165283203125\n",
      "\t Epoch Loss  2.0706090927124023\n",
      "Epoch Number :  19\n",
      "\t Training accuracy:  49.51904296875\n",
      "\t Validation accuracy  47.9925537109375\n",
      "\t Epoch Loss  2.070547103881836\n",
      "Epoch Number :  20\n",
      "\t Training accuracy:  49.494832356770836\n",
      "\t Validation accuracy  48.13232421875\n",
      "\t Epoch Loss  2.0704870223999023\n",
      "Epoch Number :  21\n",
      "\t Training accuracy:  49.49903700086806\n",
      "\t Validation accuracy  48.459879557291664\n",
      "\t Epoch Loss  2.0704305171966553\n",
      "Epoch Number :  22\n",
      "\t Training accuracy:  49.57960340711806\n",
      "\t Validation accuracy  48.719889322916664\n",
      "\t Epoch Loss  2.0703885555267334\n",
      "Epoch Number :  23\n",
      "\t Training accuracy:  49.6588134765625\n",
      "\t Validation accuracy  48.750813802083336\n",
      "\t Epoch Loss  2.0703699588775635\n",
      "Epoch Number :  24\n",
      "\t Training accuracy:  49.610866970486114\n",
      "\t Validation accuracy  48.486328125\n",
      "\t Epoch Loss  2.07035231590271\n",
      "Epoch Number :  25\n",
      "\t Training accuracy:  49.483371310763886\n",
      "\t Validation accuracy  48.015543619791664\n",
      "\t Epoch Loss  2.0703203678131104\n",
      "Epoch Number :  26\n",
      "\t Training accuracy:  49.38422309027778\n",
      "\t Validation accuracy  47.56103515625\n",
      "\t Epoch Loss  2.0702896118164062\n",
      "Epoch Number :  27\n",
      "\t Training accuracy:  49.31932237413194\n",
      "\t Validation accuracy  47.254231770833336\n",
      "\t Epoch Loss  2.0702719688415527\n",
      "Epoch Number :  28\n",
      "\t Training accuracy:  49.27971733940972\n",
      "\t Validation accuracy  47.146809895833336\n",
      "\t Epoch Loss  2.070260524749756\n",
      "Epoch Number :  29\n",
      "\t Training accuracy:  49.249267578125\n",
      "\t Validation accuracy  47.090250651041664\n",
      "\t Epoch Loss  2.0702550411224365\n",
      "Epoch Number :  30\n",
      "\t Training accuracy:  49.166598849826386\n",
      "\t Validation accuracy  46.919759114583336\n",
      "\t Epoch Loss  2.0702483654022217\n",
      "Epoch Number :  31\n",
      "\t Training accuracy:  49.053955078125\n",
      "\t Validation accuracy  46.751708984375\n",
      "\t Epoch Loss  2.070239543914795\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch Number :  32\n",
      "\t Training accuracy:  48.977864583333336\n",
      "\t Validation accuracy  46.598307291666664\n",
      "\t Epoch Loss  2.0702364444732666\n",
      "Epoch Number :  33\n",
      "\t Training accuracy:  48.970608181423614\n",
      "\t Validation accuracy  46.598307291666664\n",
      "\t Epoch Loss  2.070253849029541\n",
      "Epoch Number :  34\n",
      "\t Training accuracy:  48.958943684895836\n",
      "\t Validation accuracy  46.573486328125\n",
      "\t Epoch Loss  2.0702626705169678\n",
      "Epoch Number :  35\n",
      "\t Training accuracy:  48.965725368923614\n",
      "\t Validation accuracy  46.570841471354164\n",
      "\t Epoch Loss  2.0702784061431885\n",
      "Epoch Number :  36\n",
      "\t Training accuracy:  48.970743815104164\n",
      "\t Validation accuracy  46.5997314453125\n",
      "\t Epoch Loss  2.070300579071045\n",
      "Epoch Number :  37\n",
      "\t Training accuracy:  49.000447591145836\n",
      "\t Validation accuracy  46.622314453125\n",
      "\t Epoch Loss  2.07033634185791\n",
      "Epoch Number :  38\n",
      "\t Training accuracy:  49.0362548828125\n",
      "\t Validation accuracy  46.716715494791664\n",
      "\t Epoch Loss  2.0703938007354736\n",
      "Epoch Number :  39\n",
      "\t Training accuracy:  49.06446668836806\n",
      "\t Validation accuracy  46.844075520833336\n",
      "\t Epoch Loss  2.070498466491699\n",
      "Epoch Number :  40\n",
      "\t Training accuracy:  49.121365017361114\n",
      "\t Validation accuracy  47.093912760416664\n",
      "\t Epoch Loss  2.070681571960449\n",
      "Epoch Number :  41\n",
      "\t Training accuracy:  49.20769585503472\n",
      "\t Validation accuracy  47.315470377604164\n",
      "\t Epoch Loss  2.070958375930786\n",
      "Epoch Number :  42\n",
      "\t Training accuracy:  49.198337131076386\n",
      "\t Validation accuracy  47.500813802083336\n",
      "\t Epoch Loss  2.0712943077087402\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch Number :  43\n",
      "\t Training accuracy:  49.10881890190972\n",
      "\t Validation accuracy  47.714640299479164\n",
      "\t Epoch Loss  2.0716238021850586\n",
      "Epoch Number :  44\n",
      "\t Training accuracy:  49.10536024305556\n",
      "\t Validation accuracy  47.7520751953125\n",
      "\t Epoch Loss  2.071836233139038\n",
      "Epoch Number :  45\n",
      "\t Training accuracy:  49.112141927083336\n",
      "\t Validation accuracy  47.804158528645836\n",
      "\t Epoch Loss  2.0718603134155273\n",
      "Epoch Number :  46\n",
      "\t Training accuracy:  49.111056857638886\n",
      "\t Validation accuracy  47.8460693359375\n",
      "\t Epoch Loss  2.071884870529175\n",
      "Epoch Number :  47\n",
      "\t Training accuracy:  49.1241455078125\n",
      "\t Validation accuracy  47.876993815104164\n",
      "\t Epoch Loss  2.071908712387085\n",
      "Epoch Number :  48\n",
      "\t Training accuracy:  49.13621690538194\n",
      "\t Validation accuracy  47.9180908203125\n",
      "\t Epoch Loss  2.071931838989258\n",
      "Epoch Number :  49\n",
      "\t Training accuracy:  49.164632161458336\n",
      "\t Validation accuracy  47.9742431640625\n",
      "\t Epoch Loss  2.071953773498535\n",
      "Epoch Number :  50\n",
      "\t Training accuracy:  49.217868381076386\n",
      "\t Validation accuracy  48.058268229166664\n",
      "\t Epoch Loss  2.0719776153564453\n",
      "Epoch Number :  51\n",
      "\t Training accuracy:  49.25428602430556\n",
      "\t Validation accuracy  48.142293294270836\n",
      "\t Epoch Loss  2.0719988346099854\n",
      "Epoch Number :  52\n",
      "\t Training accuracy:  49.303927951388886\n",
      "\t Validation accuracy  48.210245768229164\n",
      "\t Epoch Loss  2.072021484375\n",
      "Epoch Number :  53\n",
      "\t Training accuracy:  49.35472276475694\n",
      "\t Validation accuracy  48.2659912109375\n",
      "\t Epoch Loss  2.0720479488372803\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch Number :  54\n",
      "\t Training accuracy:  49.399142795138886\n",
      "\t Validation accuracy  48.342692057291664\n",
      "\t Epoch Loss  2.0720720291137695\n",
      "Epoch Number :  55\n",
      "\t Training accuracy:  49.407077365451386\n",
      "\t Validation accuracy  48.346354166666664\n",
      "\t Epoch Loss  2.0720882415771484\n",
      "Epoch Number :  56\n",
      "\t Training accuracy:  49.40809461805556\n",
      "\t Validation accuracy  48.351033528645836\n",
      "\t Epoch Loss  2.072091579437256\n",
      "Epoch Number :  57\n",
      "\t Training accuracy:  49.41155327690972\n",
      "\t Validation accuracy  48.3544921875\n",
      "\t Epoch Loss  2.072094440460205\n",
      "Epoch Number :  58\n",
      "\t Training accuracy:  49.42009819878472\n",
      "\t Validation accuracy  48.357137044270836\n",
      "\t Epoch Loss  2.072096347808838\n",
      "Epoch Number :  59\n",
      "\t Training accuracy:  49.423828125\n",
      "\t Validation accuracy  48.365681966145836\n",
      "\t Epoch Loss  2.0720980167388916\n",
      "Epoch Number :  60\n",
      "\t Training accuracy:  49.426540798611114\n",
      "\t Validation accuracy  48.3740234375\n",
      "\t Epoch Loss  2.07209849357605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  61\n",
      "\t Training accuracy:  49.428914388020836\n",
      "\t Validation accuracy  48.387247721354164\n",
      "\t Epoch Loss  2.072098970413208\n",
      "Epoch Number :  62\n",
      "\t Training accuracy:  49.4305419921875\n",
      "\t Validation accuracy  48.390299479166664\n",
      "\t Epoch Loss  2.072098970413208\n",
      "Epoch Number :  63\n",
      "\t Training accuracy:  49.4329833984375\n",
      "\t Validation accuracy  48.394368489583336\n",
      "\t Epoch Loss  2.072101354598999\n",
      "Epoch Number :  64\n",
      "\t Training accuracy:  49.436238606770836\n",
      "\t Validation accuracy  48.4014892578125\n",
      "\t Epoch Loss  2.0721030235290527\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch Number :  65\n",
      "\t Training accuracy:  49.439493815104164\n",
      "\t Validation accuracy  48.408406575520836\n",
      "\t Epoch Loss  2.0721049308776855\n",
      "Epoch Number :  66\n",
      "\t Training accuracy:  49.44037543402778\n",
      "\t Validation accuracy  48.409220377604164\n",
      "\t Epoch Loss  2.07210636138916\n",
      "Epoch Number :  67\n",
      "\t Training accuracy:  49.441121419270836\n",
      "\t Validation accuracy  48.410847981770836\n",
      "\t Epoch Loss  2.07210636138916\n",
      "Epoch Number :  68\n",
      "\t Training accuracy:  49.441257052951386\n",
      "\t Validation accuracy  48.4124755859375\n",
      "\t Epoch Loss  2.07210636138916\n",
      "Epoch Number :  69\n",
      "\t Training accuracy:  49.441189236111114\n",
      "\t Validation accuracy  48.4130859375\n",
      "\t Epoch Loss  2.072106122970581\n",
      "Epoch Number :  70\n",
      "\t Training accuracy:  49.441799587673614\n",
      "\t Validation accuracy  48.412272135416664\n",
      "\t Epoch Loss  2.072105884552002\n",
      "Epoch Number :  71\n",
      "\t Training accuracy:  49.44105360243056\n",
      "\t Validation accuracy  48.412272135416664\n",
      "\t Epoch Loss  2.072105884552002\n",
      "Epoch Number :  72\n",
      "\t Training accuracy:  49.441324869791664\n",
      "\t Validation accuracy  48.413492838541664\n",
      "\t Epoch Loss  2.072105646133423\n",
      "Epoch Number :  73\n",
      "\t Training accuracy:  49.441799587673614\n",
      "\t Validation accuracy  48.4136962890625\n",
      "\t Epoch Loss  2.072105884552002\n",
      "Epoch Number :  74\n",
      "\t Training accuracy:  49.44220648871528\n",
      "\t Validation accuracy  48.414510091145836\n",
      "\t Epoch Loss  2.072106122970581\n",
      "Epoch Number :  75\n",
      "\t Training accuracy:  49.44281684027778\n",
      "\t Validation accuracy  48.4149169921875\n",
      "\t Epoch Loss  2.07210636138916\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch Number :  76\n",
      "\t Training accuracy:  49.44383409288194\n",
      "\t Validation accuracy  48.415120442708336\n",
      "\t Epoch Loss  2.07210636138916\n",
      "Epoch Number :  77\n",
      "\t Training accuracy:  49.44410536024306\n",
      "\t Validation accuracy  48.4149169921875\n",
      "\t Epoch Loss  2.0721068382263184\n",
      "Epoch Number :  78\n",
      "\t Training accuracy:  49.444173177083336\n",
      "\t Validation accuracy  48.415120442708336\n",
      "\t Epoch Loss  2.0721068382263184\n",
      "Epoch Number :  79\n",
      "\t Training accuracy:  49.444376627604164\n",
      "\t Validation accuracy  48.415120442708336\n",
      "\t Epoch Loss  2.0721068382263184\n",
      "Epoch Number :  80\n",
      "\t Training accuracy:  49.444308810763886\n",
      "\t Validation accuracy  48.414713541666664\n",
      "\t Epoch Loss  2.0721068382263184\n",
      "Epoch Number :  81\n",
      "\t Training accuracy:  49.44444444444444\n",
      "\t Validation accuracy  48.415323893229164\n",
      "\t Epoch Loss  2.0721070766448975\n",
      "Epoch Number :  82\n",
      "\t Training accuracy:  49.44390190972222\n",
      "\t Validation accuracy  48.415323893229164\n",
      "\t Epoch Loss  2.0721068382263184\n",
      "Epoch Number :  83\n",
      "\t Training accuracy:  49.444173177083336\n",
      "\t Validation accuracy  48.415120442708336\n",
      "\t Epoch Loss  2.0721070766448975\n",
      "Epoch Number :  84\n",
      "\t Training accuracy:  49.44383409288194\n",
      "\t Validation accuracy  48.415323893229164\n",
      "\t Epoch Loss  2.0721070766448975\n",
      "Epoch Number :  85\n",
      "\t Training accuracy:  49.444308810763886\n",
      "\t Validation accuracy  48.415934244791664\n",
      "\t Epoch Loss  2.0721068382263184\n",
      "Epoch Number :  86\n",
      "\t Training accuracy:  49.444240993923614\n",
      "\t Validation accuracy  48.415730794270836\n",
      "\t Epoch Loss  2.0721065998077393\n",
      "Epoch Number :  87\n",
      "\t Training accuracy:  49.444240993923614\n",
      "\t Validation accuracy  48.416341145833336\n",
      "\t Epoch Loss  2.0721065998077393\n",
      "Epoch Number :  88\n",
      "\t Training accuracy:  49.444173177083336\n",
      "\t Validation accuracy  48.415323893229164\n",
      "\t Epoch Loss  2.0721065998077393\n",
      "Epoch Number :  89\n",
      "\t Training accuracy:  49.444783528645836\n",
      "\t Validation accuracy  48.415323893229164\n",
      "\t Epoch Loss  2.0721065998077393\n",
      "Epoch Number :  90\n",
      "\t Training accuracy:  49.444783528645836\n",
      "\t Validation accuracy  48.41552734375\n",
      "\t Epoch Loss  2.0721065998077393\n",
      "Epoch Number :  91\n",
      "\t Training accuracy:  49.444376627604164\n",
      "\t Validation accuracy  48.415730794270836\n",
      "\t Epoch Loss  2.0721065998077393\n",
      "Epoch Number :  92\n",
      "\t Training accuracy:  49.444240993923614\n",
      "\t Validation accuracy  48.41552734375\n",
      "\t Epoch Loss  2.0721065998077393\n",
      "Epoch Number :  93\n",
      "\t Training accuracy:  49.44444444444444\n",
      "\t Validation accuracy  48.415934244791664\n",
      "\t Epoch Loss  2.0721065998077393\n",
      "Epoch Number :  94\n",
      "\t Training accuracy:  49.44444444444444\n",
      "\t Validation accuracy  48.416341145833336\n",
      "\t Epoch Loss  2.07210636138916\n",
      "Epoch Number :  95\n",
      "\t Training accuracy:  49.444376627604164\n",
      "\t Validation accuracy  48.416341145833336\n",
      "\t Epoch Loss  2.0721065998077393\n",
      "Epoch Number :  96\n",
      "\t Training accuracy:  49.44444444444444\n",
      "\t Validation accuracy  48.417154947916664\n",
      "\t Epoch Loss  2.07210636138916\n",
      "Epoch Number :  97\n",
      "\t Training accuracy:  49.44471571180556\n",
      "\t Validation accuracy  48.416951497395836\n",
      "\t Epoch Loss  2.0721065998077393\n",
      "Epoch Number :  98\n",
      "\t Training accuracy:  49.44471571180556\n",
      "\t Validation accuracy  48.417561848958336\n",
      "\t Epoch Loss  2.0721068382263184\n",
      "Epoch Number :  99\n",
      "\t Training accuracy:  49.444986979166664\n",
      "\t Validation accuracy  48.416748046875\n",
      "\t Epoch Loss  2.0721068382263184\n"
     ]
    }
   ],
   "source": [
    "# Train network \n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.PoissonNLLLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.SmoothL1Loss() #interesting ... but does not converge\n",
    "#criterion = nn.MSELoss() #0.83 but unstable\n",
    "\n",
    "if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "    train_target = Variable(preprocessed_input_train_target).long()  # keep long tensors\n",
    "    validation_target = Variable(preprocessed_input_validation_target, volatile=True).long() # convert to float\n",
    "    Noutputs = 2\n",
    "    \n",
    "elif isinstance(criterion, nn.NLLLoss):\n",
    "    train_target = Variable(preprocessed_input_train_target)  # keep long tensors\n",
    "    validation_target = Variable(preprocessed_input_validation_target, volatile=True) # convert to float\n",
    "    Noutputs = 2\n",
    "    \n",
    "else:\n",
    "    train_target = Variable(preprocessed_input_train_target.float()) # convert to float\n",
    "    validation_target = Variable(preprocessed_input_validation_target.float(), volatile=True ) # convert to float\n",
    "    Noutputs = 1\n",
    "    \n",
    "Nbatches = int(math.ceil(Ntrain/batch_size)) #batch_size is defined above\n",
    "Nepochs = 100\n",
    "Nrep = 1\n",
    "        \n",
    "#model = conv3DNet(grid_size, Noutputs, batch_size)\n",
    "#model = conv3DNet(grid_size, Noutputs, batch_size)\n",
    "#model = conv3DNet(grid_size, Noutputs, batch_size)\n",
    "#model = conv3DNet(grid_size, Noutputs, batch_size)\n",
    "model = UnetGenerator_3d(in_dim=1, out_dim=Noutputs, num_filter=4)\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.90)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "#optimizer = optim.Adagrad(model.parameters())\n",
    "#optimizer = optim.Adamax(model.parameters())\n",
    "#optimizer = optim.ASGD(model.parameters())\n",
    "#optimizer = optim.RMSprop(model.parameters())\n",
    "#optimizer = optim.Rprop(model.parameters())\n",
    " \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, verbose=True) #Reduces the learning rate if it did not decreased by more than 10^-4 in 10 steps\n",
    "\n",
    "train_errors = torch.Tensor(Nepochs).zero_()\n",
    "validation_errors = torch.Tensor(Nepochs).zero_()\n",
    "\n",
    "ep_loss = torch.Tensor(Nepochs).zero_()\n",
    "\n",
    "for i_ep in range(Nepochs):\n",
    "    for b_start in range(0, Ntrain, batch_size):\n",
    "        bsize_eff = batch_size - max(0, b_start+batch_size-Ntrain)  # boundary case\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        output = model(train_input.narrow(0, b_start, bsize_eff))\n",
    "        if isinstance(criterion, nn.CrossEntropyLoss) or isinstance(criterion, nn.NLLLoss):\n",
    "            batch_loss = criterion(output, train_target.narrow(0, b_start, bsize_eff))\n",
    "        else:\n",
    "            #if delta model is chosen\n",
    "            #batch_loss = criterion(output.view(bsize_eff*Noutputs), train_target.narrow(0, b_start, bsize_eff))\n",
    "            batch_loss = criterion(output.view(bsize_eff,grid_size,grid_size,grid_size), train_target.narrow(0, b_start, bsize_eff))\n",
    "        ep_loss[i_ep] += batch_loss.data[0]\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step(ep_loss[i_ep])\n",
    "\n",
    "    nb_train_errs = compute_nb_errors(model, train_input, train_target, batch_size, criterion)\n",
    "    nb_validation_errs = compute_nb_errors(model, validation_input, validation_target, batch_size, criterion)\n",
    "\n",
    "    Ntrain_nb = Ntrain*grid_size**3\n",
    "    Nvalidation_nb = Nvalidation*grid_size**3\n",
    "    print(\"Epoch Number : \", i_ep)\n",
    "    print(\"\\t Training accuracy: \", (100*(Ntrain_nb-nb_train_errs)/Ntrain_nb))\n",
    "    print(\"\\t Validation accuracy \",(100*(Nvalidation_nb-nb_validation_errs)/Nvalidation_nb))\n",
    "\n",
    "    print(\"\\t Epoch Loss \", ep_loss[i_ep])\n",
    "\n",
    "    train_errors[i_ep] = nb_train_errs\n",
    "    validation_errors[i_ep] = nb_validation_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "   1   1   1  ...    1   1   1\n",
      "   1   1   1  ...    1   1   1\n",
      "   1   1   1  ...    1   1   1\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "(1 ,.,.) = \n",
      "   1   1   1  ...    1   1   1\n",
      "   1   1   1  ...    1   1   1\n",
      "   1   1   1  ...    1   1   1\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "(2 ,.,.) = \n",
      "   1   1   1  ...    1   1   1\n",
      "   1   1   1  ...    1   1   1\n",
      "   1   1   1  ...    1   1   1\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "...\n",
      "\n",
      "(29,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "(30,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "(31,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "[torch.LongTensor of size 32x32x32]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_target[43,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 2, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  6.6668  6.6668  6.6667  ...   6.6666  6.6667  6.6667\n",
      "  6.6664  6.6663  6.6662  ...   6.6667  6.6667  6.6666\n",
      "  6.6652  6.6637  6.6616  ...   6.6667  6.6666  6.6666\n",
      "           ...             ⋱             ...          \n",
      "  6.6668  6.6667  6.6669  ...   6.6667  6.6667  6.6666\n",
      "  6.6665  6.6667  6.6668  ...   6.6667  6.6668  6.6667\n",
      "  6.6669  6.6668  6.6668  ...   6.6666  6.6666  6.6666\n",
      "\n",
      "(1 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  6.6666  6.6667  6.6665  ...   6.6666  6.6666  6.6666\n",
      "  6.6661  6.6663  6.6665  ...   6.6668  6.6668  6.6668\n",
      "  6.6633  6.6635  6.6599  ...   6.6667  6.6666  6.6666\n",
      "           ...             ⋱             ...          \n",
      "  6.6664  6.6661  6.6664  ...   6.6665  6.6665  6.6667\n",
      "  6.6663  6.6666  6.6669  ...   6.6666  6.6667  6.6668\n",
      "  6.6667  6.6665  6.6665  ...   6.6665  6.6666  6.6666\n",
      "\n",
      "(2 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  6.6668  6.6667  6.6664  ...   6.6666  6.6667  6.6667\n",
      "  6.6655  6.6649  6.6635  ...   6.6669  6.6669  6.6667\n",
      "  6.6587  6.6517  6.6456  ...   6.6669  6.6667  6.6666\n",
      "           ...             ⋱             ...          \n",
      "  6.6670  6.6678  6.6679  ...   6.6667  6.6664  6.6666\n",
      "  6.6665  6.6670  6.6675  ...   6.6664  6.6668  6.6668\n",
      "  6.6666  6.6667  6.6666  ...   6.6664  6.6666  6.6666\n",
      "...\n",
      "\n",
      "(29,.,.) = \n",
      "1.00000e-02 *\n",
      "  6.6667  6.6667  6.6667  ...   6.6667  6.6667  6.6667\n",
      "  6.6667  6.6666  6.6665  ...   6.6667  6.6668  6.6666\n",
      "  6.6667  6.6666  6.6666  ...   6.6668  6.6667  6.6667\n",
      "           ...             ⋱             ...          \n",
      "  6.6657  6.6650  6.6653  ...   6.6669  6.6665  6.6668\n",
      "  6.6662  6.6658  6.6666  ...   6.6665  6.6666  6.6667\n",
      "  6.6665  6.6659  6.6662  ...   6.6665  6.6665  6.6666\n",
      "\n",
      "(30,.,.) = \n",
      "1.00000e-02 *\n",
      "  6.6667  6.6667  6.6666  ...   6.6667  6.6667  6.6667\n",
      "  6.6666  6.6666  6.6665  ...   6.6667  6.6668  6.6667\n",
      "  6.6667  6.6666  6.6666  ...   6.6668  6.6668  6.6668\n",
      "           ...             ⋱             ...          \n",
      "  6.6651  6.6643  6.6641  ...   6.6669  6.6663  6.6665\n",
      "  6.6659  6.6657  6.6661  ...   6.6663  6.6665  6.6664\n",
      "  6.6661  6.6655  6.6646  ...   6.6664  6.6664  6.6665\n",
      "\n",
      "(31,.,.) = \n",
      "1.00000e-02 *\n",
      "  6.6667  6.6666  6.6667  ...   6.6667  6.6667  6.6667\n",
      "  6.6666  6.6667  6.6666  ...   6.6667  6.6667  6.6666\n",
      "  6.6666  6.6666  6.6666  ...   6.6667  6.6667  6.6667\n",
      "           ...             ⋱             ...          \n",
      "  6.6656  6.6656  6.6648  ...   6.6666  6.6666  6.6667\n",
      "  6.6666  6.6663  6.6668  ...   6.6665  6.6667  6.6667\n",
      "  6.6665  6.6663  6.6659  ...   6.6666  6.6666  6.6666\n",
      "[torch.FloatTensor of size 32x32x32]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output[14,1,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"voxels = np.array(output[4,:,:,:])\\n\\n# and plot everything\\nfig = plt.figure(figsize=(10,10))\\nax = fig.gca(projection='3d')\\nax.voxels(voxels)\\nfig.savefig('VoxelizedFinal.png')\\nfig.show()\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''voxels = np.array(output[4,:,:,:])\n",
    "\n",
    "# and plot everything\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.voxels(voxels)\n",
    "fig.savefig('VoxelizedFinal.png')\n",
    "fig.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def summary(input_size, model):\\n        def register_hook(module):\\n            def hook(module, input, output):\\n                class_name = str(module.__class__).split(\\'.\\')[-1].split(\"\\'\")[0]\\n                module_idx = len(summary)\\n\\n                m_key = \\'%s-%i\\' % (class_name, module_idx+1)\\n                summary[m_key] = OrderedDict()\\n                summary[m_key][\\'input_shape\\'] = list(input[0].size())\\n                summary[m_key][\\'input_shape\\'][0] = -1\\n                summary[m_key][\\'output_shape\\'] = list(output.size())\\n                summary[m_key][\\'output_shape\\'][0] = -1\\n\\n                params = 0\\n                if hasattr(module, \\'weight\\'):\\n                    params += th.prod(th.LongTensor(list(module.weight.size())))\\n                    if module.weight.requires_grad:\\n                        summary[m_key][\\'trainable\\'] = True\\n                    else:\\n                        summary[m_key][\\'trainable\\'] = False\\n                if hasattr(module, \\'bias\\'):\\n                    params +=  th.prod(th.LongTensor(list(module.bias.size())))\\n                summary[m_key][\\'nb_params\\'] = params\\n                \\n            if not isinstance(module, nn.Sequential) and                not isinstance(module, nn.ModuleList) and                not (module == model):\\n                hooks.append(module.register_forward_hook(hook))\\n                \\n        dtype = th.cuda.FloatTensor\\n        \\n        # check if there are multiple inputs to the network\\n        if isinstance(input_size[0], (list, tuple)):\\n            x = [Variable(th.rand(1,*in_size)).type(dtype) for in_size in input_size]\\n        else:\\n            x = Variable(th.rand(1,*input_size)).type(dtype)\\n            \\n        print(x.shape)\\n        print(type(x[0]))\\n        # create properties\\n        summary = OrderedDict()\\n        hooks = []\\n        # register hook\\n        model.apply(register_hook)\\n        # make a forward pass\\n        model(x)\\n        # remove these hooks\\n        for h in hooks:\\n            h.remove()\\n\\n        print(\\'----------------------------------------------------------------\\')\\n        line_new = \\'{:>20}  {:>25} {:>15}\\'.format(\\'Layer (type)\\', \\'Output Shpae\\', \\'Param #\\')\\n        print(line_new)\\n        print(\\'================================================================\\')\\n        total_params = 0\\n        trainable_params = 0\\n        for layer in summary:\\n            ## input_shape, output_shape, trainable, nb_params\\n            line_new = \\'{:>20}  {:>25} {:>15}\\'.format(layer, summary[layer][\\'output_shape\\'], summary[layer][\\'nb_params\\'])\\n            total_params += summary[layer][\\'nb_params\\']\\n            if \\'trainable\\' in summary[layer]:\\n                if summary[layer][\\'trainable\\'] == True:\\n                    trainable_params += summary[layer][\\'nb_params\\']\\n            print(line_new)\\n        print(\\'================================================================\\')\\n        print(\\'Total params: \\' + str(total_params))\\n        print(\\'Trainable params: \\' + str(trainable_params))\\n        print(\\'Non-trainable params: \\' + str(total_params - trainable_params))\\n        print(\\'----------------------------------------------------------------\\')\\n        return summary'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def summary(input_size, model):\n",
    "        def register_hook(module):\n",
    "            def hook(module, input, output):\n",
    "                class_name = str(module.__class__).split('.')[-1].split(\"'\")[0]\n",
    "                module_idx = len(summary)\n",
    "\n",
    "                m_key = '%s-%i' % (class_name, module_idx+1)\n",
    "                summary[m_key] = OrderedDict()\n",
    "                summary[m_key]['input_shape'] = list(input[0].size())\n",
    "                summary[m_key]['input_shape'][0] = -1\n",
    "                summary[m_key]['output_shape'] = list(output.size())\n",
    "                summary[m_key]['output_shape'][0] = -1\n",
    "\n",
    "                params = 0\n",
    "                if hasattr(module, 'weight'):\n",
    "                    params += th.prod(th.LongTensor(list(module.weight.size())))\n",
    "                    if module.weight.requires_grad:\n",
    "                        summary[m_key]['trainable'] = True\n",
    "                    else:\n",
    "                        summary[m_key]['trainable'] = False\n",
    "                if hasattr(module, 'bias'):\n",
    "                    params +=  th.prod(th.LongTensor(list(module.bias.size())))\n",
    "                summary[m_key]['nb_params'] = params\n",
    "                \n",
    "            if not isinstance(module, nn.Sequential) and \\\n",
    "               not isinstance(module, nn.ModuleList) and \\\n",
    "               not (module == model):\n",
    "                hooks.append(module.register_forward_hook(hook))\n",
    "                \n",
    "        dtype = th.cuda.FloatTensor\n",
    "        \n",
    "        # check if there are multiple inputs to the network\n",
    "        if isinstance(input_size[0], (list, tuple)):\n",
    "            x = [Variable(th.rand(1,*in_size)).type(dtype) for in_size in input_size]\n",
    "        else:\n",
    "            x = Variable(th.rand(1,*input_size)).type(dtype)\n",
    "            \n",
    "        print(x.shape)\n",
    "        print(type(x[0]))\n",
    "        # create properties\n",
    "        summary = OrderedDict()\n",
    "        hooks = []\n",
    "        # register hook\n",
    "        model.apply(register_hook)\n",
    "        # make a forward pass\n",
    "        model(x)\n",
    "        # remove these hooks\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "        print('----------------------------------------------------------------')\n",
    "        line_new = '{:>20}  {:>25} {:>15}'.format('Layer (type)', 'Output Shpae', 'Param #')\n",
    "        print(line_new)\n",
    "        print('================================================================')\n",
    "        total_params = 0\n",
    "        trainable_params = 0\n",
    "        for layer in summary:\n",
    "            ## input_shape, output_shape, trainable, nb_params\n",
    "            line_new = '{:>20}  {:>25} {:>15}'.format(layer, summary[layer]['output_shape'], summary[layer]['nb_params'])\n",
    "            total_params += summary[layer]['nb_params']\n",
    "            if 'trainable' in summary[layer]:\n",
    "                if summary[layer]['trainable'] == True:\n",
    "                    trainable_params += summary[layer]['nb_params']\n",
    "            print(line_new)\n",
    "        print('================================================================')\n",
    "        print('Total params: ' + str(total_params))\n",
    "        print('Trainable params: ' + str(trainable_params))\n",
    "        print('Non-trainable params: ' + str(total_params - trainable_params))\n",
    "        print('----------------------------------------------------------------')\n",
    "        return summary'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
