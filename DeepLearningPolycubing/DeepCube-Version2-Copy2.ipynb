{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"nbagg\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from utility import *\n",
    "from models import *\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define from where to load the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "polycube_path = \"/Users/davidcleres/DeepShape/Polycubing-Automated/Generated-Cars/\"\n",
    "polycube_files = [f for f in listdir(polycube_path) if isfile(join(polycube_path, f))]\n",
    "\n",
    "voxelized_mesh_path = \"/Users/davidcleres/DeepShape/Polycubing-Automated/voxelizedMeshes/\"\n",
    "voxelized_mesh_files = [f for f in listdir(voxelized_mesh_path) if isfile(join(voxelized_mesh_path, f))]\n",
    "\n",
    "voxelizedFiles = []\n",
    "polycubedFiles = []\n",
    "\n",
    "for f in voxelized_mesh_files: \n",
    "    if f[-13:] == \"voxelized.txt\":\n",
    "        voxelizedFiles = np.hstack((voxelizedFiles, f))\n",
    "    \n",
    "for f in polycube_files:\n",
    "    if f[-14:] == \"finalCubes.txt\":\n",
    "        polycubedFiles = np.hstack((polycubedFiles, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 32\n",
    "voxelized_train_input, polycube_target=loadData(grid_size, polycube_path, voxelized_mesh_path, voxelizedFiles, polycubedFiles, loadFromScratch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a training and validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10 \n",
    "validation_size=50\n",
    "preprocessed_input_train, preprocessed_input_validation, preprocessed_input_train_target, preprocessed_input_validation_target = preprocessing_train(voxelized_train_input, polycube_target, validation_size, False, False)\n",
    "\n",
    "preprocessed_input_train = torch.from_numpy(preprocessed_input_train)\n",
    "preprocessed_input_validation = torch.from_numpy(preprocessed_input_validation)\n",
    "preprocessed_input_train_target = torch.from_numpy(preprocessed_input_train_target)\n",
    "preprocessed_input_validation_target = torch.from_numpy(preprocessed_input_validation_target)\n",
    "\n",
    "Ntrain = len(preprocessed_input_train[:,0,0,0,0]) \n",
    "Nvalidation = len(preprocessed_input_validation[:,0,0,0,0])\n",
    "image_size = 32\n",
    "\n",
    "train_input = np.array(preprocessed_input_train.view(Ntrain, 1,image_size, image_size, image_size))\n",
    "validation_input = np.array(preprocessed_input_validation.view(Nvalidation, 1,image_size, image_size, image_size))\n",
    "\n",
    "labels_train = np.array(preprocessed_input_train_target.view(Ntrain, 1, image_size, image_size, image_size))\n",
    "labels_validation = np.array(preprocessed_input_validation_target.view(Nvalidation, 1,image_size, image_size, image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the target \n",
    "Find the middle of the cube and the distance to the x, y and z boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input torch.Size([550, 1, 32, 32, 32])\n",
      "labels_train_cube_coords torch.Size([550, 1, 9])\n",
      "validation_input torch.Size([50, 1, 32, 32, 32])\n",
      "labels_validation_cube_coords torch.Size([50, 1, 9])\n"
     ]
    }
   ],
   "source": [
    "labels_train_cube_coords = np.zeros((len(train_input[:,0,0,0,0]), 1, 9))\n",
    "for i in range (len(train_input[:,0,0,0,0])):\n",
    "    #solution 1 - the loss is calculed based on the 9 labels \n",
    "    delta_x_left, delta_x_right, center_x, delta_y_left, delta_y_right, center_y, delta_z_left, delta_z_right, center_z = find_center_and_delta(labels_train[i, 0, :, :, :], grid_size=32)\n",
    "    labels_train_cube_coords[i,0,0] = delta_x_left\n",
    "    labels_train_cube_coords[i,0,1] = delta_x_right\n",
    "    labels_train_cube_coords[i,0,2] = center_x\n",
    "    labels_train_cube_coords[i,0,3] = delta_y_left\n",
    "    labels_train_cube_coords[i,0,4] = delta_y_right\n",
    "    labels_train_cube_coords[i,0,5] = center_y\n",
    "    labels_train_cube_coords[i,0,6] = delta_z_left\n",
    "    labels_train_cube_coords[i,0,7] = delta_z_right\n",
    "    labels_train_cube_coords[i,0,8] = center_z\n",
    "    \n",
    "labels_validation_cube_coords = np.zeros((len(labels_validation[:,0,0,0,0]), 1, 9))\n",
    "for i in range (len((validation_input[:,0,0,0,0]))):\n",
    "    #solution 1 - the loss is calculed based on the 9 labels \n",
    "    delta_x_left, delta_x_right, center_x, delta_y_left, delta_y_right, center_y, delta_z_left, delta_z_right, center_z = find_center_and_delta(labels_train[i, 0, :, :, :], grid_size=32)\n",
    "    labels_validation_cube_coords[i, 0,0] = delta_x_left\n",
    "    labels_validation_cube_coords[i, 0,1] = delta_x_right\n",
    "    labels_validation_cube_coords[i, 0,2] = center_x\n",
    "    labels_validation_cube_coords[i, 0,3] = delta_y_left\n",
    "    labels_validation_cube_coords[i, 0,4] = delta_y_right\n",
    "    labels_validation_cube_coords[i, 0,5] = center_y\n",
    "    labels_validation_cube_coords[i, 0,6] = delta_z_left\n",
    "    labels_validation_cube_coords[i, 0,7] = delta_z_right\n",
    "    labels_validation_cube_coords[i, 0,8] = center_z\n",
    "    \n",
    "    \n",
    "labels_validation_cube_coords = torch.from_numpy(labels_validation_cube_coords)\n",
    "labels_train_cube_coords = torch.from_numpy(labels_train_cube_coords)\n",
    "train_input = torch.from_numpy(train_input)\n",
    "validation_input = torch.from_numpy(validation_input)\n",
    "\n",
    "print('train_input', train_input.shape)\n",
    "print('labels_train_cube_coords', labels_train_cube_coords.shape)\n",
    "\n",
    "print('validation_input', validation_input.shape)\n",
    "print('labels_validation_cube_coords', labels_validation_cube_coords.shape)\n",
    "\n",
    "    \n",
    "    #solution 2 - the loss is calculated from the cube generated thanks to the learned coordinates \n",
    "    #output = build_cube(delta_x_left, delta_x_right, center_x, delta_y_left, delta_y_right, center_y, delta_z_left, delta_z_right, center_z, grid_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  0\n",
      "\t Training accuracy:  98.24242424242425\n",
      "\t Validation accuracy  80.22222222222223\n",
      "\t Epoch Loss  33754.07421875\n",
      "Epoch Number :  1\n",
      "\t Training accuracy:  98.28282828282828\n",
      "\t Validation accuracy  80.66666666666667\n",
      "\t Epoch Loss  20435.40234375\n",
      "Epoch Number :  2\n",
      "\t Training accuracy:  98.20202020202021\n",
      "\t Validation accuracy  80.22222222222223\n",
      "\t Epoch Loss  17527.751953125\n",
      "Epoch Number :  3\n",
      "\t Training accuracy:  98.20202020202021\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  26744.4453125\n",
      "Epoch Number :  4\n",
      "\t Training accuracy:  98.24242424242425\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  24280.392578125\n",
      "Epoch Number :  5\n",
      "\t Training accuracy:  98.20202020202021\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  15553.021484375\n",
      "Epoch Number :  6\n",
      "\t Training accuracy:  98.28282828282828\n",
      "\t Validation accuracy  80.22222222222223\n",
      "\t Epoch Loss  30318.640625\n",
      "Epoch Number :  7\n",
      "\t Training accuracy:  98.20202020202021\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  19800.548828125\n",
      "Epoch Number :  8\n",
      "\t Training accuracy:  98.28282828282828\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  5850.13916015625\n",
      "Epoch Number :  9\n",
      "\t Training accuracy:  98.28282828282828\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  4033.705810546875\n",
      "Epoch Number :  10\n",
      "\t Training accuracy:  98.3030303030303\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  3310.66015625\n",
      "Epoch Number :  11\n",
      "\t Training accuracy:  98.38383838383838\n",
      "\t Validation accuracy  80.44444444444444\n",
      "\t Epoch Loss  3111.099853515625\n",
      "Epoch Number :  12\n",
      "\t Training accuracy:  98.18181818181819\n",
      "\t Validation accuracy  81.11111111111111\n",
      "\t Epoch Loss  3583.066650390625\n",
      "Epoch Number :  13\n",
      "\t Training accuracy:  98.26262626262626\n",
      "\t Validation accuracy  80.44444444444444\n",
      "\t Epoch Loss  3165.75634765625\n",
      "Epoch Number :  14\n",
      "\t Training accuracy:  98.22222222222223\n",
      "\t Validation accuracy  80.22222222222223\n",
      "\t Epoch Loss  2972.80322265625\n",
      "Epoch Number :  15\n",
      "\t Training accuracy:  98.3030303030303\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  1906.8988037109375\n",
      "Epoch Number :  16\n",
      "\t Training accuracy:  98.24242424242425\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  1987.6168212890625\n",
      "Epoch Number :  17\n",
      "\t Training accuracy:  98.46464646464646\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  2388.551513671875\n",
      "Epoch Number :  18\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  1917.0086669921875\n",
      "Epoch Number :  19\n",
      "\t Training accuracy:  98.3030303030303\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  1851.46142578125\n",
      "Epoch Number :  20\n",
      "\t Training accuracy:  98.26262626262626\n",
      "\t Validation accuracy  80.66666666666667\n",
      "\t Epoch Loss  1861.7830810546875\n",
      "Epoch Number :  21\n",
      "\t Training accuracy:  98.34343434343434\n",
      "\t Validation accuracy  81.11111111111111\n",
      "\t Epoch Loss  2362.063232421875\n",
      "Epoch Number :  22\n",
      "\t Training accuracy:  98.34343434343434\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  1233.864013671875\n",
      "Epoch Number :  23\n",
      "\t Training accuracy:  98.3030303030303\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  1440.44189453125\n",
      "Epoch Number :  24\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  2223.34765625\n",
      "Epoch Number :  25\n",
      "\t Training accuracy:  98.26262626262626\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  1302.6654052734375\n",
      "Epoch Number :  26\n",
      "\t Training accuracy:  98.28282828282828\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  2059.1650390625\n",
      "Epoch Number :  27\n",
      "\t Training accuracy:  98.32323232323232\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  2204.22509765625\n",
      "Epoch Number :  28\n",
      "\t Training accuracy:  98.3030303030303\n",
      "\t Validation accuracy  80.22222222222223\n",
      "\t Epoch Loss  3499.506103515625\n",
      "Epoch Number :  29\n",
      "\t Training accuracy:  98.42424242424242\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  3277.369140625\n",
      "Epoch Number :  30\n",
      "\t Training accuracy:  98.34343434343434\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  2524.451416015625\n",
      "Epoch Number :  31\n",
      "\t Training accuracy:  98.28282828282828\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  2531.45068359375\n",
      "Epoch Number :  32\n",
      "\t Training accuracy:  98.44444444444444\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  1567.4910888671875\n",
      "Epoch Number :  33\n",
      "\t Training accuracy:  98.26262626262626\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  1140.289794921875\n",
      "Epoch Number :  34\n",
      "\t Training accuracy:  98.36363636363636\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  886.2449951171875\n",
      "Epoch Number :  35\n",
      "\t Training accuracy:  98.5050505050505\n",
      "\t Validation accuracy  80.66666666666667\n",
      "\t Epoch Loss  1044.443359375\n",
      "Epoch Number :  36\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  753.5575561523438\n",
      "Epoch Number :  37\n",
      "\t Training accuracy:  98.26262626262626\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  801.2224731445312\n",
      "Epoch Number :  38\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  759.40283203125\n",
      "Epoch Number :  39\n",
      "\t Training accuracy:  98.44444444444444\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  646.6805419921875\n",
      "Epoch Number :  40\n",
      "\t Training accuracy:  98.52525252525253\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  666.2890625\n",
      "Epoch Number :  41\n",
      "\t Training accuracy:  98.42424242424242\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  727.7037963867188\n",
      "Epoch Number :  42\n",
      "\t Training accuracy:  98.46464646464646\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  824.6431274414062\n",
      "Epoch Number :  43\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  970.8094482421875\n",
      "Epoch Number :  44\n",
      "\t Training accuracy:  98.52525252525253\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  712.8267211914062\n",
      "Epoch Number :  45\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  462.7911376953125\n",
      "Epoch Number :  46\n",
      "\t Training accuracy:  98.42424242424242\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  464.3673400878906\n",
      "Epoch Number :  47\n",
      "\t Training accuracy:  98.36363636363636\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  417.6977844238281\n",
      "Epoch Number :  48\n",
      "\t Training accuracy:  98.5050505050505\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  562.345703125\n",
      "Epoch Number :  49\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  417.2610168457031\n",
      "Epoch Number :  50\n",
      "\t Training accuracy:  98.42424242424242\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  386.68011474609375\n",
      "Epoch Number :  51\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  401.0735778808594\n",
      "Epoch Number :  52\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  613.544921875\n",
      "Epoch Number :  53\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  503.27435302734375\n",
      "Epoch Number :  54\n",
      "\t Training accuracy:  98.44444444444444\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  569.31982421875\n",
      "Epoch Number :  55\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  514.0858154296875\n",
      "Epoch Number :  56\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  443.01983642578125\n",
      "Epoch Number :  57\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  435.5644836425781\n",
      "Epoch Number :  58\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  470.4898376464844\n",
      "Epoch Number :  59\n",
      "\t Training accuracy:  98.52525252525253\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  713.5751342773438\n",
      "Epoch Number :  60\n",
      "\t Training accuracy:  98.42424242424242\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  808.8275756835938\n",
      "Epoch Number :  61\n",
      "\t Training accuracy:  98.52525252525253\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  611.418212890625\n",
      "Epoch Number :  62\n",
      "\t Training accuracy:  98.44444444444444\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  542.9502563476562\n",
      "Epoch Number :  63\n",
      "\t Training accuracy:  98.5050505050505\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  344.2749328613281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  64\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  314.4631652832031\n",
      "Epoch Number :  65\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  311.6839599609375\n",
      "Epoch Number :  66\n",
      "\t Training accuracy:  98.44444444444444\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  312.5277404785156\n",
      "Epoch Number :  67\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  330.8912048339844\n",
      "Epoch Number :  68\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  425.1662292480469\n",
      "Epoch Number :  69\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  286.9247741699219\n",
      "Epoch Number :  70\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  286.9089660644531\n",
      "Epoch Number :  71\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  252.00807189941406\n",
      "Epoch Number :  72\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  255.71678161621094\n",
      "Epoch Number :  73\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  225.3356170654297\n",
      "Epoch Number :  74\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  247.61117553710938\n",
      "Epoch Number :  75\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  81.11111111111111\n",
      "\t Epoch Loss  221.48138427734375\n",
      "Epoch Number :  76\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  189.36074829101562\n",
      "Epoch Number :  77\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  165.0540771484375\n",
      "Epoch Number :  78\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  178.1666259765625\n",
      "Epoch Number :  79\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  169.09877014160156\n",
      "Epoch Number :  80\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  151.6937713623047\n",
      "Epoch Number :  81\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  166.22398376464844\n",
      "Epoch Number :  82\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  166.40933227539062\n",
      "Epoch Number :  83\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  147.7576141357422\n",
      "Epoch Number :  84\n",
      "\t Training accuracy:  98.86868686868686\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  122.54224395751953\n",
      "Epoch Number :  85\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  143.5373992919922\n",
      "Epoch Number :  86\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  134.0528106689453\n",
      "Epoch Number :  87\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  138.51596069335938\n",
      "Epoch Number :  88\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  149.58018493652344\n",
      "Epoch Number :  89\n",
      "\t Training accuracy:  98.9090909090909\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  162.4092559814453\n",
      "Epoch Number :  90\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  162.9534149169922\n",
      "Epoch Number :  91\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  138.12989807128906\n",
      "Epoch Number :  92\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  141.41921997070312\n",
      "Epoch Number :  93\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  140.19432067871094\n",
      "Epoch Number :  94\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  135.6736602783203\n",
      "Epoch Number :  95\n",
      "\t Training accuracy:  98.86868686868686\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  136.3115234375\n",
      "Epoch Number :  96\n",
      "\t Training accuracy:  99.01010101010101\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  124.8050765991211\n",
      "Epoch Number :  97\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  128.1331329345703\n",
      "Epoch Number :  98\n",
      "\t Training accuracy:  99.01010101010101\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  121.12808990478516\n",
      "Epoch Number :  99\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  142.4388427734375\n",
      "Epoch Number :  100\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  86.88888888888889\n",
      "\t Epoch Loss  142.52830505371094\n",
      "Epoch Number :  101\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  124.63678741455078\n",
      "Epoch Number :  102\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  110.36834716796875\n",
      "Epoch Number :  103\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  107.10711669921875\n",
      "Epoch Number :  104\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  110.59819030761719\n",
      "Epoch Number :  105\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  109.78030395507812\n",
      "Epoch Number :  106\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  94.04141998291016\n",
      "Epoch Number :  107\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  94.8671646118164\n",
      "Epoch Number :  108\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  93.99385070800781\n",
      "Epoch Number :  109\n",
      "\t Training accuracy:  99.17171717171718\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  90.14462280273438\n",
      "Epoch Number :  110\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  112.28949737548828\n",
      "Epoch Number :  111\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  98.8709487915039\n",
      "Epoch Number :  112\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  88.22222222222223\n",
      "\t Epoch Loss  122.346923828125\n",
      "Epoch Number :  113\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  117.55731201171875\n",
      "Epoch Number :  114\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  104.71665954589844\n",
      "Epoch Number :  115\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  120.2472152709961\n",
      "Epoch Number :  116\n",
      "\t Training accuracy:  99.01010101010101\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  94.0858154296875\n",
      "Epoch Number :  117\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  106.83071899414062\n",
      "Epoch Number :  118\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  101.38601684570312\n",
      "Epoch Number :  119\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  87.02704620361328\n",
      "Epoch Number :  120\n",
      "\t Training accuracy:  99.13131313131314\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  103.77902221679688\n",
      "Epoch Number :  121\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  88.84968566894531\n",
      "Epoch Number :  122\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  87.77867889404297\n",
      "Epoch Number :  123\n",
      "\t Training accuracy:  99.21212121212122\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  80.36241149902344\n",
      "Epoch Number :  124\n",
      "\t Training accuracy:  99.1919191919192\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  79.9513168334961\n",
      "Epoch Number :  125\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  75.99482727050781\n",
      "Epoch Number :  126\n",
      "\t Training accuracy:  98.9090909090909\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  88.67987060546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  127\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  81.1088638305664\n",
      "Epoch Number :  128\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  81.7378921508789\n",
      "Epoch Number :  129\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  107.31300354003906\n",
      "Epoch Number :  130\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  71.89588165283203\n",
      "Epoch Number :  131\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  85.82560729980469\n",
      "Epoch Number :  132\n",
      "\t Training accuracy:  99.01010101010101\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  74.1005859375\n",
      "Epoch Number :  133\n",
      "\t Training accuracy:  99.35353535353535\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  88.41265869140625\n",
      "Epoch Number :  134\n",
      "\t Training accuracy:  99.25252525252525\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  84.2659683227539\n",
      "Epoch Number :  135\n",
      "\t Training accuracy:  99.17171717171718\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  75.81319427490234\n",
      "Epoch Number :  136\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  77.46370697021484\n",
      "Epoch Number :  137\n",
      "\t Training accuracy:  99.01010101010101\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  83.1446762084961\n",
      "Epoch Number :  138\n",
      "\t Training accuracy:  99.1919191919192\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  68.15151977539062\n",
      "Epoch Number :  139\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  78.40795135498047\n",
      "Epoch Number :  140\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  96.0391845703125\n",
      "Epoch Number :  141\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  92.46819305419922\n",
      "Epoch Number :  142\n",
      "\t Training accuracy:  98.96969696969697\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  98.9345703125\n",
      "Epoch Number :  143\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  100.79244232177734\n",
      "Epoch Number :  144\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  115.44098663330078\n",
      "Epoch Number :  145\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  133.5483856201172\n",
      "Epoch Number :  146\n",
      "\t Training accuracy:  99.17171717171718\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  159.810791015625\n",
      "Epoch Number :  147\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  174.0546875\n",
      "Epoch Number :  148\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  151.27169799804688\n",
      "Epoch Number :  149\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  170.1448974609375\n",
      "Epoch Number :  150\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  200.71945190429688\n",
      "Epoch Number :  151\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  291.7491149902344\n",
      "Epoch Number :  152\n",
      "\t Training accuracy:  98.86868686868686\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  156.17298889160156\n",
      "Epoch Number :  153\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  241.49502563476562\n",
      "Epoch Number :  154\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  347.5722351074219\n",
      "Epoch Number :  155\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  318.1013488769531\n",
      "Epoch Number :  156\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  203.419921875\n",
      "Epoch Number :  157\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  168.71482849121094\n",
      "Epoch Number :  158\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  110.9839859008789\n",
      "Epoch Number :  159\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  116.29438018798828\n",
      "Epoch Number :  160\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  105.52523803710938\n",
      "Epoch Number :  161\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  93.25447845458984\n",
      "Epoch Number :  162\n",
      "\t Training accuracy:  98.9090909090909\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  100.41670227050781\n",
      "Epoch Number :  163\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  111.63529968261719\n",
      "Epoch Number :  164\n",
      "\t Training accuracy:  99.1919191919192\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  121.20044708251953\n",
      "Epoch Number :  165\n",
      "\t Training accuracy:  98.86868686868686\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  116.1321792602539\n",
      "Epoch Number :  166\n",
      "\t Training accuracy:  99.25252525252525\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  106.38662719726562\n",
      "Epoch Number :  167\n",
      "\t Training accuracy:  99.23232323232324\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  85.44076538085938\n",
      "Epoch Number :  168\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  74.61371612548828\n",
      "Epoch Number :  169\n",
      "\t Training accuracy:  99.33333333333333\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  77.93219757080078\n",
      "Epoch Number :  170\n",
      "\t Training accuracy:  99.27272727272727\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  86.42327117919922\n",
      "Epoch Number :  171\n",
      "\t Training accuracy:  98.86868686868686\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  91.86199188232422\n",
      "Epoch Number :  172\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  108.57330322265625\n",
      "Epoch Number :  173\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  155.63995361328125\n",
      "Epoch Number :  174\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  181.79200744628906\n",
      "Epoch Number :  175\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  167.70550537109375\n",
      "Epoch Number :  176\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  125.35404205322266\n",
      "Epoch Number :  177\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  146.6891632080078\n",
      "Epoch Number :  178\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  129.2665252685547\n",
      "Epoch Number :  179\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  165.583984375\n",
      "Epoch Number :  180\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  134.53575134277344\n",
      "Epoch Number :  181\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  147.57757568359375\n",
      "Epoch Number :  182\n",
      "\t Training accuracy:  98.5050505050505\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  159.40098571777344\n",
      "Epoch Number :  183\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  162.67291259765625\n",
      "Epoch Number :  184\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  137.3645782470703\n",
      "Epoch Number :  185\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  167.56240844726562\n",
      "Epoch Number :  186\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  128.02801513671875\n",
      "Epoch Number :  187\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  139.3108367919922\n",
      "Epoch Number :  188\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  132.63755798339844\n",
      "Epoch Number :  189\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  156.13815307617188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  190\n",
      "\t Training accuracy:  99.17171717171718\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  141.29843139648438\n",
      "Epoch Number :  191\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  120.933349609375\n",
      "Epoch Number :  192\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  157.2489471435547\n",
      "Epoch Number :  193\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  154.22056579589844\n",
      "Epoch Number :  194\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  158.089111328125\n",
      "Epoch Number :  195\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  148.28475952148438\n",
      "Epoch Number :  196\n",
      "\t Training accuracy:  98.86868686868686\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  192.19244384765625\n",
      "Epoch Number :  197\n",
      "\t Training accuracy:  98.9090909090909\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  107.49920654296875\n",
      "Epoch Number :  198\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  94.52948760986328\n",
      "Epoch Number :  199\n",
      "\t Training accuracy:  98.96969696969697\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  95.58401489257812\n",
      "Epoch Number :  200\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  125.1176528930664\n",
      "Epoch Number :  201\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  122.8071517944336\n",
      "Epoch Number :  202\n",
      "\t Training accuracy:  99.01010101010101\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  141.5498504638672\n",
      "Epoch Number :  203\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  140.74911499023438\n",
      "Epoch Number :  204\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  157.7788543701172\n",
      "Epoch Number :  205\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  189.00827026367188\n",
      "Epoch Number :  206\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  129.74490356445312\n",
      "Epoch Number :  207\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  187.08804321289062\n",
      "Epoch Number :  208\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  145.1058807373047\n",
      "Epoch Number :  209\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  164.47463989257812\n",
      "Epoch Number :  210\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  165.89662170410156\n",
      "Epoch Number :  211\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  202.034912109375\n",
      "Epoch Number :  212\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  87.11111111111111\n",
      "\t Epoch Loss  155.06057739257812\n",
      "Epoch Number :  213\n",
      "\t Training accuracy:  98.9090909090909\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  110.33544921875\n",
      "Epoch Number :  214\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  114.57579803466797\n",
      "Epoch Number :  215\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  125.32075500488281\n",
      "Epoch Number :  216\n",
      "\t Training accuracy:  98.9090909090909\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  117.13090515136719\n",
      "Epoch Number :  217\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  120.25762939453125\n",
      "Epoch Number :  218\n",
      "\t Training accuracy:  99.01010101010101\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  107.33909606933594\n",
      "Epoch Number :  219\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  100.32266235351562\n",
      "Epoch Number :  220\n",
      "\t Training accuracy:  98.86868686868686\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  102.29794311523438\n",
      "Epoch Number :  221\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  119.20789337158203\n",
      "Epoch Number :  222\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  156.3794403076172\n",
      "Epoch Number :  223\n",
      "\t Training accuracy:  98.96969696969697\n",
      "\t Validation accuracy  87.11111111111111\n",
      "\t Epoch Loss  138.36477661132812\n",
      "Epoch Number :  224\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  124.731201171875\n",
      "Epoch Number :  225\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  113.39104461669922\n",
      "Epoch Number :  226\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  129.3779296875\n",
      "Epoch Number :  227\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  211.5268096923828\n",
      "Epoch Number :  228\n",
      "\t Training accuracy:  98.46464646464646\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  210.5321044921875\n",
      "Epoch Number :  229\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  134.1143798828125\n",
      "Epoch Number :  230\n",
      "\t Training accuracy:  98.96969696969697\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  126.06324005126953\n",
      "Epoch Number :  231\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  170.90521240234375\n",
      "Epoch Number :  232\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  128.67022705078125\n",
      "Epoch Number :  233\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  156.43283081054688\n",
      "Epoch Number :  234\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  195.938232421875\n",
      "Epoch Number :  235\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  177.92987060546875\n",
      "Epoch Number :  236\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  225.4068145751953\n",
      "Epoch Number :  237\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  159.27584838867188\n",
      "Epoch Number :  238\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  159.22183227539062\n",
      "Epoch Number :  239\n",
      "\t Training accuracy:  98.86868686868686\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  154.6675262451172\n",
      "Epoch Number :  240\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  170.91481018066406\n",
      "Epoch Number :  241\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  191.430419921875\n",
      "Epoch Number :  242\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  221.1663818359375\n",
      "Epoch Number :  243\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  157.6798858642578\n",
      "Epoch Number :  244\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  173.8609619140625\n",
      "Epoch Number :  245\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  171.1862335205078\n",
      "Epoch Number :  246\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  175.1817169189453\n",
      "Epoch Number :  247\n",
      "\t Training accuracy:  98.4040404040404\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  202.17074584960938\n",
      "Epoch Number :  248\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  215.68307495117188\n",
      "Epoch Number :  249\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  282.6068420410156\n",
      "Epoch Number :  250\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  211.49183654785156\n",
      "Epoch Number :  251\n",
      "\t Training accuracy:  98.52525252525253\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  256.5792236328125\n",
      "Epoch Number :  252\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  237.2899932861328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  253\n",
      "\t Training accuracy:  98.5050505050505\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  186.09986877441406\n",
      "Epoch Number :  254\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  213.1182861328125\n",
      "Epoch Number :  255\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  271.652587890625\n",
      "Epoch Number :  256\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  226.54669189453125\n",
      "Epoch Number :  257\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  206.49447631835938\n",
      "Epoch Number :  258\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  271.9506530761719\n",
      "Epoch Number :  259\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  334.603271484375\n",
      "Epoch Number :  260\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  198.80810546875\n",
      "Epoch Number :  261\n",
      "\t Training accuracy:  98.44444444444444\n",
      "\t Validation accuracy  81.11111111111111\n",
      "\t Epoch Loss  269.18194580078125\n",
      "Epoch Number :  262\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  397.72418212890625\n",
      "Epoch Number :  263\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  264.2405090332031\n",
      "Epoch Number :  264\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  219.46627807617188\n",
      "Epoch Number :  265\n",
      "\t Training accuracy:  98.5050505050505\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  235.28868103027344\n",
      "Epoch Number :  266\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  318.8810729980469\n",
      "Epoch Number :  267\n",
      "\t Training accuracy:  98.42424242424242\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  348.9844055175781\n",
      "Epoch Number :  268\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  382.7710266113281\n",
      "Epoch Number :  269\n",
      "\t Training accuracy:  98.38383838383838\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  275.1547546386719\n",
      "Epoch Number :  270\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  329.6220397949219\n",
      "Epoch Number :  271\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  269.2141418457031\n",
      "Epoch Number :  272\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  398.5558166503906\n",
      "Epoch Number :  273\n",
      "\t Training accuracy:  98.38383838383838\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  477.2149353027344\n",
      "Epoch Number :  274\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  301.0746765136719\n",
      "Epoch Number :  275\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  402.4369201660156\n",
      "Epoch Number :  276\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  81.11111111111111\n",
      "\t Epoch Loss  289.6209411621094\n",
      "Epoch Number :  277\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  308.1690979003906\n",
      "Epoch Number :  278\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  81.11111111111111\n",
      "\t Epoch Loss  390.6649169921875\n",
      "Epoch Number :  279\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  314.77947998046875\n",
      "Epoch Number :  280\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  337.9857177734375\n",
      "Epoch Number :  281\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  217.3421173095703\n",
      "Epoch Number :  282\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  193.7628173828125\n",
      "Epoch Number :  283\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  225.9910125732422\n",
      "Epoch Number :  284\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  180.26573181152344\n",
      "Epoch Number :  285\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  242.28248596191406\n",
      "Epoch Number :  286\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  194.12564086914062\n",
      "Epoch Number :  287\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  260.73974609375\n",
      "Epoch Number :  288\n",
      "\t Training accuracy:  98.28282828282828\n",
      "\t Validation accuracy  80.66666666666667\n",
      "\t Epoch Loss  216.2240447998047\n",
      "Epoch Number :  289\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  240.1742401123047\n",
      "Epoch Number :  290\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  257.85107421875\n",
      "Epoch Number :  291\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  166.63113403320312\n",
      "Epoch Number :  292\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  214.76318359375\n",
      "Epoch Number :  293\n",
      "\t Training accuracy:  98.42424242424242\n",
      "\t Validation accuracy  80.66666666666667\n",
      "\t Epoch Loss  143.64430236816406\n",
      "Epoch Number :  294\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  163.58065795898438\n",
      "Epoch Number :  295\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  181.00003051757812\n",
      "Epoch Number :  296\n",
      "\t Training accuracy:  98.5050505050505\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  177.78614807128906\n",
      "Epoch Number :  297\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  154.38775634765625\n",
      "Epoch Number :  298\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  186.3646240234375\n",
      "Epoch Number :  299\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  159.41326904296875\n",
      "Epoch Number :  300\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  157.5612335205078\n",
      "Epoch Number :  301\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  147.43643188476562\n",
      "Epoch Number :  302\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  80.44444444444444\n",
      "\t Epoch Loss  151.59425354003906\n",
      "Epoch Number :  303\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  156.4932098388672\n",
      "Epoch Number :  304\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  160.73255920410156\n",
      "Epoch Number :  305\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  81.11111111111111\n",
      "\t Epoch Loss  157.07749938964844\n",
      "Epoch Number :  306\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  160.03077697753906\n",
      "Epoch Number :  307\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  147.7040557861328\n",
      "Epoch Number :  308\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  155.84402465820312\n",
      "Epoch Number :  309\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  146.04246520996094\n",
      "Epoch Number :  310\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  143.23464965820312\n",
      "Epoch Number :  311\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  156.93319702148438\n",
      "Epoch Number :  312\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  196.7843017578125\n",
      "Epoch Number :  313\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  148.71080017089844\n",
      "Epoch Number :  314\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  202.55450439453125\n",
      "Epoch Number :  315\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  182.5264892578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  316\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  173.42672729492188\n",
      "Epoch Number :  317\n",
      "\t Training accuracy:  98.36363636363636\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  205.9748077392578\n",
      "Epoch Number :  318\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  213.65208435058594\n",
      "Epoch Number :  319\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  178.0428924560547\n",
      "Epoch Number :  320\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  210.1643829345703\n",
      "Epoch Number :  321\n",
      "\t Training accuracy:  98.44444444444444\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  241.74856567382812\n",
      "Epoch Number :  322\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  232.1937713623047\n",
      "Epoch Number :  323\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  208.8462371826172\n",
      "Epoch Number :  324\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  185.23558044433594\n",
      "Epoch Number :  325\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  219.38345336914062\n",
      "Epoch Number :  326\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  155.62762451171875\n",
      "Epoch Number :  327\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  133.9413299560547\n",
      "Epoch Number :  328\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  125.5715560913086\n",
      "Epoch Number :  329\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  132.80792236328125\n",
      "Epoch Number :  330\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  244.13917541503906\n",
      "Epoch Number :  331\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  184.8936004638672\n",
      "Epoch Number :  332\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  234.34585571289062\n",
      "Epoch Number :  333\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  229.20042419433594\n",
      "Epoch Number :  334\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  229.14646911621094\n",
      "Epoch Number :  335\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  245.32586669921875\n",
      "Epoch Number :  336\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  209.21861267089844\n",
      "Epoch Number :  337\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  231.17906188964844\n",
      "Epoch Number :  338\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  253.82138061523438\n",
      "Epoch Number :  339\n",
      "\t Training accuracy:  98.4040404040404\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  255.43380737304688\n",
      "Epoch Number :  340\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  254.9217071533203\n",
      "Epoch Number :  341\n",
      "\t Training accuracy:  98.42424242424242\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  253.9884796142578\n",
      "Epoch Number :  342\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  285.8990173339844\n",
      "Epoch Number :  343\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  281.8753967285156\n",
      "Epoch Number :  344\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  303.2682189941406\n",
      "Epoch Number :  345\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  354.4667053222656\n",
      "Epoch Number :  346\n",
      "\t Training accuracy:  98.4040404040404\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  319.3915710449219\n",
      "Epoch Number :  347\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  221.8614044189453\n",
      "Epoch Number :  348\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  166.710205078125\n",
      "Epoch Number :  349\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  91.63436126708984\n",
      "Epoch Number :  350\n",
      "\t Training accuracy:  99.13131313131314\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  80.16588592529297\n",
      "Epoch Number :  351\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  72.72341918945312\n",
      "Epoch Number :  352\n",
      "\t Training accuracy:  98.86868686868686\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  71.47455596923828\n",
      "Epoch Number :  353\n",
      "\t Training accuracy:  98.52525252525253\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  81.7716293334961\n",
      "Epoch Number :  354\n",
      "\t Training accuracy:  99.13131313131314\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  75.5185317993164\n",
      "Epoch Number :  355\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  65.89706420898438\n",
      "Epoch Number :  356\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  65.61662292480469\n",
      "Epoch Number :  357\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  62.90825653076172\n",
      "Epoch Number :  358\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  71.05856323242188\n",
      "Epoch Number :  359\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  80.24098205566406\n",
      "Epoch Number :  360\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  99.82634735107422\n",
      "Epoch Number :  361\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  162.37997436523438\n",
      "Epoch Number :  362\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  185.66278076171875\n",
      "Epoch Number :  363\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  230.35694885253906\n",
      "Epoch Number :  364\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  182.20474243164062\n",
      "Epoch Number :  365\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  286.7259826660156\n",
      "Epoch Number :  366\n",
      "\t Training accuracy:  98.4040404040404\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  274.7977294921875\n",
      "Epoch Number :  367\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  217.2316436767578\n",
      "Epoch Number :  368\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  80.66666666666667\n",
      "\t Epoch Loss  205.4115447998047\n",
      "Epoch Number :  369\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  242.32850646972656\n",
      "Epoch Number :  370\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  81.11111111111111\n",
      "\t Epoch Loss  319.9021911621094\n",
      "Epoch Number :  371\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  80.66666666666667\n",
      "\t Epoch Loss  335.35076904296875\n",
      "Epoch Number :  372\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  226.54531860351562\n",
      "Epoch Number :  373\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  97.4060287475586\n",
      "Epoch Number :  374\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  83.13408660888672\n",
      "Epoch Number :  375\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  68.1386489868164\n",
      "Epoch Number :  376\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  66.1893310546875\n",
      "Epoch Number :  377\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  72.90145111083984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  378\n",
      "\t Training accuracy:  98.96969696969697\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  63.84920120239258\n",
      "Epoch Number :  379\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  76.99207305908203\n",
      "Epoch Number :  380\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  71.00199127197266\n",
      "Epoch Number :  381\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  70.27627563476562\n",
      "Epoch Number :  382\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  136.13980102539062\n",
      "Epoch Number :  383\n",
      "\t Training accuracy:  98.44444444444444\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  220.52955627441406\n",
      "Epoch Number :  384\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  165.1508026123047\n",
      "Epoch Number :  385\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  203.0103302001953\n",
      "Epoch Number :  386\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  230.2704620361328\n",
      "Epoch Number :  387\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  186.31463623046875\n",
      "Epoch Number :  388\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  187.69778442382812\n",
      "Epoch Number :  389\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  239.84744262695312\n",
      "Epoch Number :  390\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  192.638916015625\n",
      "Epoch Number :  391\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  175.65548706054688\n",
      "Epoch Number :  392\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  201.2923583984375\n",
      "Epoch Number :  393\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  196.97068786621094\n",
      "Epoch Number :  394\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  268.54034423828125\n",
      "Epoch Number :  395\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  213.94720458984375\n",
      "Epoch Number :  396\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  214.86148071289062\n",
      "Epoch Number :  397\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  244.3687744140625\n",
      "Epoch Number :  398\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  275.06732177734375\n",
      "Epoch Number :  399\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  240.98101806640625\n",
      "Epoch Number :  400\n",
      "\t Training accuracy:  98.38383838383838\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  269.9158630371094\n",
      "Epoch Number :  401\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  236.62115478515625\n",
      "Epoch Number :  402\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  210.3701934814453\n",
      "Epoch Number :  403\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  140.48196411132812\n",
      "Epoch Number :  404\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  206.19000244140625\n",
      "Epoch Number :  405\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  209.99957275390625\n",
      "Epoch Number :  406\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  235.87449645996094\n",
      "Epoch Number :  407\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  234.58078002929688\n",
      "Epoch Number :  408\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  277.4829406738281\n",
      "Epoch Number :  409\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  294.1988830566406\n",
      "Epoch Number :  410\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  339.2196044921875\n",
      "Epoch Number :  411\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  252.01097106933594\n",
      "Epoch Number :  412\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  284.5232238769531\n",
      "Epoch Number :  413\n",
      "\t Training accuracy:  98.52525252525253\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  305.42095947265625\n",
      "Epoch Number :  414\n",
      "\t Training accuracy:  98.4040404040404\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  216.71107482910156\n",
      "Epoch Number :  415\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  242.30886840820312\n",
      "Epoch Number :  416\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  308.3260803222656\n",
      "Epoch Number :  417\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  281.52056884765625\n",
      "Epoch Number :  418\n",
      "\t Training accuracy:  98.4040404040404\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  280.82568359375\n",
      "Epoch Number :  419\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  239.9270782470703\n",
      "Epoch Number :  420\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  287.6310119628906\n"
     ]
    }
   ],
   "source": [
    "# Train network \n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.PoissonNLLLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.SmoothL1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_input = Variable(train_input).float()\n",
    "validation_input = Variable(validation_input).float()\n",
    "\n",
    "if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "    train_target = Variable(labels_train_cube_coords)  # keep long tensors\n",
    "    validation_target = Variable(labels_validation_cube_coords, \n",
    "    uires_grad=False) # convert to float\n",
    "    Noutputs = 18\n",
    "    \n",
    "elif isinstance(criterion, nn.NLLLoss):\n",
    "    train_target = Variable(labels_train_cube_coords)  # keep long tensors\n",
    "    validation_target = Variable(labels_validation_cube_coords, requires_grad=False) # convert to float\n",
    "    Noutputs = 18\n",
    "    \n",
    "else:\n",
    "    train_target = Variable(labels_train_cube_coords, requires_grad=False).float() # convert to float\n",
    "    validation_target = Variable(labels_validation_cube_coords, requires_grad=False).float() # convert to float\n",
    "    Noutputs = 9\n",
    "\n",
    "Nbatches = int(math.ceil(Ntrain/batch_size))\n",
    "Nepochs = 500\n",
    "#seeds = list(range(15)) #Test 15 different seeds but always the seeds from 0 to 15 so that the weights are always initialized in a reproducible way\n",
    "#Nrep = len(seeds)\n",
    "Nrep = 1\n",
    "\n",
    "train_errors = torch.Tensor(Nrep, Nepochs).zero_()\n",
    "test_errors = torch.Tensor(Nrep, Nepochs).zero_()\n",
    "validation_errors = torch.Tensor(Nrep, Nepochs).zero_()\n",
    "ep_loss = torch.Tensor(Nrep, Nepochs).zero_()\n",
    "\n",
    "for i_rep in range(Nrep):    \n",
    "    #print('Repetition', seeds[i_rep])\n",
    "    #torch.manual_seed(seeds[i_rep])\n",
    "    \n",
    "    #model = conv2DNet_1(Nchannels, Nsamples_100, Noutputs) #from litterature EEG-Net coorected\n",
    "    model = conv2DNet_2(Noutputs)  #from Temporal - Spatial; 4 Filters Model - Best performing model with accuracy 0.83 in average on the validation set\n",
    "    #model = conv2DNet_3(Noutputs) #from Temporal - Spatial; 64 Filters Model\n",
    "    #model = conv2DNet_4(Noutputs) #from Temporal - Spatial; 128 Filters Model\n",
    "    \n",
    "    #optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.50)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    #optimizer = optim.Adagrad(model.parameters())\n",
    "    #optimizer = optim.Adamax(model.parameters())\n",
    "    #optimizer = optim.ASGD(model.parameters())\n",
    "    #optimizer = optim.RMSprop(model.parameters())\n",
    "    #optimizer = optim.Rprop(model.parameters())\n",
    "    \n",
    "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, verbose=True) #Reduces the learning rate if it did not decreased by more than 10^-4 in 10 steps\n",
    "\n",
    "    for i_ep in range(Nepochs):\n",
    "        for b_start in range(0, Ntrain, batch_size):\n",
    "            bsize_eff = batch_size - max(0, b_start+batch_size-Ntrain)  # boundary case\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            output = model(train_input.narrow(0, b_start, bsize_eff))\n",
    "            if isinstance(criterion, nn.CrossEntropyLoss) or isinstance(criterion, nn.NLLLoss):\n",
    "                batch_loss = criterion(output, train_target.narrow(0, b_start, bsize_eff))\n",
    "            else:\n",
    "                batch_loss = criterion(output.view(bsize_eff*Noutputs), train_target.narrow(0, b_start, bsize_eff))\n",
    "            ep_loss[i_rep, i_ep] += batch_loss.data[0]\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        #scheduler.step(ep_loss[i_rep, i_ep])\n",
    "        \n",
    "        nb_train_errs = compute_nb_errors_delta(model, train_input, train_target, batch_size, criterion)\n",
    "        nb_validation_errs = compute_nb_errors_delta(model, validation_input, validation_target, batch_size, criterion)\n",
    "        \n",
    "        print(\"Epoch Number : \", i_ep)\n",
    "        print(\"\\t Training accuracy: \", (100*(Ntrain*Noutputs-nb_train_errs)/(Ntrain*Noutputs)))\n",
    "        print(\"\\t Validation accuracy \",(100*(Nvalidation*Noutputs-nb_validation_errs)/(Nvalidation*Noutputs)))\n",
    "        \n",
    "        print(\"\\t Epoch Loss \", ep_loss[i_rep, i_ep])\n",
    "        \n",
    "        train_errors[i_rep, i_ep] = nb_train_errs\n",
    "        validation_errors[i_rep, i_ep] = nb_validation_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK SUMMARY & RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = 100*(Ntrain-np.array(train_errors))/Ntrain\n",
    "val_accuracy = 100*(Nvalidation-np.array(validation_errors))/Nvalidation\n",
    "np.save('training_accuracy_Adamax', train_accuracy)\n",
    "np.save('validation_accuracy_Adamax', val_accuracy)\n",
    "\n",
    "stddev_train_errors = np.std(train_accuracy, axis=0)\n",
    "stddev_val_errors = np.std(val_accuracy, axis=0)\n",
    "\n",
    "mean_train_errors = np.mean(train_accuracy, axis=0)\n",
    "mean_val_errors = np.mean(val_accuracy, axis=0)\n",
    "\n",
    "epoch = list(range(50))\n",
    "plt.plot(epoch, mean_train_errors)\n",
    "plt.plot(epoch, mean_val_errors)\n",
    "plt.fill_between(epoch, mean_train_errors+stddev_train_errors, mean_train_errors-stddev_train_errors, alpha=0.5)\n",
    "plt.fill_between(epoch, mean_val_errors+stddev_val_errors, mean_val_errors-stddev_val_errors, alpha=0.5)\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Accuracy in %')\n",
    "plt.legend(['train', 'validation', 'test'])\n",
    "\n",
    "print(\"Training accuracy {:4.3g}%+-{}\".format(mean_train_errors[-1], stddev_train_errors[-1]))\n",
    "print(\"Validation accuracy {:4.3g}%+-{}\".format(mean_val_errors[-1], stddev_val_errors[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target.narrow(0, b_start, bsize_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
