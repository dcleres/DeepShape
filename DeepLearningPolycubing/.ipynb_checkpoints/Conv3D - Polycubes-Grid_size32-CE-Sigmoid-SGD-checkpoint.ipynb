{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"nbagg\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from utility import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import torchvision.utils as v_utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "\n",
    "polycube_path = \"/Users/davidcleres/DeepShape/Polycubing-Automated/Generated-Cars/\"\n",
    "polycube_files = [f for f in listdir(polycube_path) if isfile(join(polycube_path, f))]\n",
    "\n",
    "voxelized_mesh_path = \"/Users/davidcleres/DeepShape/Polycubing-Automated/voxelizedMeshes/\"\n",
    "voxelized_mesh_files = [f for f in listdir(voxelized_mesh_path) if isfile(join(voxelized_mesh_path, f))]\n",
    "\n",
    "voxelizedFiles = []\n",
    "polycubedFiles = []\n",
    "\n",
    "for f in voxelized_mesh_files: \n",
    "    if f[-13:] == \"voxelized.txt\":\n",
    "        voxelizedFiles = np.hstack((voxelizedFiles, f))\n",
    "    \n",
    "for f in polycube_files:\n",
    "    if f[-14:] == \"finalCubes.txt\":\n",
    "        polycubedFiles = np.hstack((polycubedFiles, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the global parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size=32\n",
    "batch_size=15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the tensor to a text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxelized_train_input, polycube_target=loadData(grid_size, polycube_path, voxelized_mesh_path, voxelizedFiles, polycubedFiles, loadFromScratch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_input_train, preprocessed_input_validation, preprocessed_input_train_target, preprocessed_input_validation_target = preprocessing_train(voxelized_train_input, polycube_target,batch_size, False, False)\n",
    "\n",
    "preprocessed_input_train = torch.from_numpy(preprocessed_input_train)\n",
    "preprocessed_input_validation = torch.from_numpy(preprocessed_input_validation)\n",
    "preprocessed_input_train_target = torch.from_numpy(preprocessed_input_train_target)\n",
    "preprocessed_input_validation_target = torch.from_numpy(preprocessed_input_validation_target)\n",
    "\n",
    "Ntrain = len(preprocessed_input_train[:, 0,0,0,0]) \n",
    "Nvalidation = len(preprocessed_input_validation[:,0,0,0,0])\n",
    "\n",
    "train_input = Variable(preprocessed_input_train.view(Ntrain, 1, grid_size, grid_size, grid_size).float())\n",
    "validation_input = Variable(preprocessed_input_validation.view(Nvalidation, 1, grid_size, grid_size, grid_size).float())\n",
    "\n",
    "labels_train = preprocessed_input_train_target.float()\n",
    "labels_validation = preprocessed_input_validation_target.float()\n",
    "\n",
    "print('train', train_input.shape)\n",
    "print('validation', validation_input.shape)\n",
    "print('train_target', labels_train.shape)\n",
    "print('validation_target', labels_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network \n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.PoissonNLLLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.SmoothL1Loss() #interesting ... but does not converge\n",
    "#criterion = nn.MSELoss() #0.83 but unstable\n",
    "\n",
    "if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "    train_target = Variable(preprocessed_input_train_target).long()  # keep long tensors\n",
    "    validation_target = Variable(preprocessed_input_validation_target, volatile=True).long() # convert to float\n",
    "    Noutputs = 2\n",
    "    \n",
    "elif isinstance(criterion, nn.NLLLoss):\n",
    "    train_target = Variable(preprocessed_input_train_target)  # keep long tensors\n",
    "    validation_target = Variable(preprocessed_input_validation_target, volatile=True) # convert to float\n",
    "    Noutputs = 2\n",
    "    \n",
    "else:\n",
    "    train_target = Variable(preprocessed_input_train_target.float()) # convert to float\n",
    "    validation_target = Variable(preprocessed_input_validation_target.float(), volatile=True ) # convert to float\n",
    "    Noutputs = 1\n",
    "    \n",
    "Nbatches = int(math.ceil(Ntrain/batch_size)) #batch_size is defined above\n",
    "Nepochs = 1000\n",
    "Nrep = 1\n",
    "        \n",
    "#model = conv3DNet(grid_size, Noutputs, batch_size)\n",
    "#model = conv3DNet(grid_size, Noutputs, batch_size)\n",
    "#model = conv3DNet(grid_size, Noutputs, batch_size)\n",
    "#model = conv3DNet(grid_size, Noutputs, batch_size)\n",
    "model = UnetGenerator_3d(in_dim=1, out_dim=Noutputs, num_filter=4)\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.90)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "#optimizer = optim.Adagrad(model.parameters())\n",
    "#optimizer = optim.Adamax(model.parameters())\n",
    "#optimizer = optim.ASGD(model.parameters())\n",
    "#optimizer = optim.RMSprop(model.parameters())\n",
    "#optimizer = optim.Rprop(model.parameters())\n",
    " \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, verbose=True) #Reduces the learning rate if it did not decreased by more than 10^-4 in 10 steps\n",
    "\n",
    "train_errors = torch.Tensor(Nepochs).zero_()\n",
    "validation_errors = torch.Tensor(Nepochs).zero_()\n",
    "\n",
    "ep_loss = torch.Tensor(Nepochs).zero_()\n",
    "\n",
    "for i_ep in range(Nepochs):\n",
    "    for b_start in range(0, Ntrain, batch_size):\n",
    "        bsize_eff = batch_size - max(0, b_start+batch_size-Ntrain)  # boundary case\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        output = model(train_input.narrow(0, b_start, bsize_eff))\n",
    "        if isinstance(criterion, nn.CrossEntropyLoss) or isinstance(criterion, nn.NLLLoss):\n",
    "            batch_loss = criterion(output, train_target.narrow(0, b_start, bsize_eff))\n",
    "        else:\n",
    "            #if delta model is chosen\n",
    "            #batch_loss = criterion(output.view(bsize_eff*Noutputs), train_target.narrow(0, b_start, bsize_eff))\n",
    "            batch_loss = criterion(output.view(bsize_eff,grid_size,grid_size,grid_size), train_target.narrow(0, b_start, bsize_eff))\n",
    "        ep_loss[i_ep] += batch_loss.data[0]\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step(ep_loss[i_ep])\n",
    "\n",
    "    nb_train_errs = compute_nb_errors(model, train_input, train_target, batch_size, criterion)\n",
    "    nb_validation_errs = compute_nb_errors(model, validation_input, validation_target, batch_size, criterion)\n",
    "\n",
    "    Ntrain_nb = Ntrain*grid_size**3\n",
    "    Nvalidation_nb = Nvalidation*grid_size**3\n",
    "    print(\"Epoch Number : \", i_ep)\n",
    "    print(\"\\t Training accuracy: \", (100*(Ntrain_nb-nb_train_errs)/Ntrain_nb))\n",
    "    print(\"\\t Validation accuracy \",(100*(Nvalidation_nb-nb_validation_errs)/Nvalidation_nb))\n",
    "\n",
    "    print(\"\\t Epoch Loss \", ep_loss[i_ep])\n",
    "\n",
    "    train_errors[i_ep] = nb_train_errs\n",
    "    validation_errors[i_ep] = nb_validation_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_target[44,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[14,1,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = np.array(100*(Ntrain_nb-train_errors)/Ntrain_nb)\n",
    "validation_accurcy = np.array(100*(Nvalidation_nb-validation_errors)/Nvalidation_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_accuracy)\n",
    "plt.plot(validation_accurcy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_visualisation = output[14,1,:,:,:].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxels = np.array(test_visualisation.data)\n",
    "\n",
    "# and plot everything\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.voxels(voxels)\n",
    "fig.savefig('VoxelizedFinal.png')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def summary(input_size, model):\n",
    "        def register_hook(module):\n",
    "            def hook(module, input, output):\n",
    "                class_name = str(module.__class__).split('.')[-1].split(\"'\")[0]\n",
    "                module_idx = len(summary)\n",
    "\n",
    "                m_key = '%s-%i' % (class_name, module_idx+1)\n",
    "                summary[m_key] = OrderedDict()\n",
    "                summary[m_key]['input_shape'] = list(input[0].size())\n",
    "                summary[m_key]['input_shape'][0] = -1\n",
    "                summary[m_key]['output_shape'] = list(output.size())\n",
    "                summary[m_key]['output_shape'][0] = -1\n",
    "\n",
    "                params = 0\n",
    "                if hasattr(module, 'weight'):\n",
    "                    params += th.prod(th.LongTensor(list(module.weight.size())))\n",
    "                    if module.weight.requires_grad:\n",
    "                        summary[m_key]['trainable'] = True\n",
    "                    else:\n",
    "                        summary[m_key]['trainable'] = False\n",
    "                if hasattr(module, 'bias'):\n",
    "                    params +=  th.prod(th.LongTensor(list(module.bias.size())))\n",
    "                summary[m_key]['nb_params'] = params\n",
    "                \n",
    "            if not isinstance(module, nn.Sequential) and \\\n",
    "               not isinstance(module, nn.ModuleList) and \\\n",
    "               not (module == model):\n",
    "                hooks.append(module.register_forward_hook(hook))\n",
    "                \n",
    "        dtype = th.cuda.FloatTensor\n",
    "        \n",
    "        # check if there are multiple inputs to the network\n",
    "        if isinstance(input_size[0], (list, tuple)):\n",
    "            x = [Variable(th.rand(1,*in_size)).type(dtype) for in_size in input_size]\n",
    "        else:\n",
    "            x = Variable(th.rand(1,*input_size)).type(dtype)\n",
    "            \n",
    "        print(x.shape)\n",
    "        print(type(x[0]))\n",
    "        # create properties\n",
    "        summary = OrderedDict()\n",
    "        hooks = []\n",
    "        # register hook\n",
    "        model.apply(register_hook)\n",
    "        # make a forward pass\n",
    "        model(x)\n",
    "        # remove these hooks\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "        print('----------------------------------------------------------------')\n",
    "        line_new = '{:>20}  {:>25} {:>15}'.format('Layer (type)', 'Output Shpae', 'Param #')\n",
    "        print(line_new)\n",
    "        print('================================================================')\n",
    "        total_params = 0\n",
    "        trainable_params = 0\n",
    "        for layer in summary:\n",
    "            ## input_shape, output_shape, trainable, nb_params\n",
    "            line_new = '{:>20}  {:>25} {:>15}'.format(layer, summary[layer]['output_shape'], summary[layer]['nb_params'])\n",
    "            total_params += summary[layer]['nb_params']\n",
    "            if 'trainable' in summary[layer]:\n",
    "                if summary[layer]['trainable'] == True:\n",
    "                    trainable_params += summary[layer]['nb_params']\n",
    "            print(line_new)\n",
    "        print('================================================================')\n",
    "        print('Total params: ' + str(total_params))\n",
    "        print('Trainable params: ' + str(trainable_params))\n",
    "        print('Non-trainable params: ' + str(total_params - trainable_params))\n",
    "        print('----------------------------------------------------------------')\n",
    "        return summary'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
