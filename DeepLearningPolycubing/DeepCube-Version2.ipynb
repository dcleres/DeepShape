{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"nbagg\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from utility import *\n",
    "from models import *\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define from where to load the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "polycube_path = \"/Users/davidcleres/DeepShape/Polycubing-Automated/Generated-Cars/\"\n",
    "polycube_files = [f for f in listdir(polycube_path) if isfile(join(polycube_path, f))]\n",
    "\n",
    "voxelized_mesh_path = \"/Users/davidcleres/DeepShape/Polycubing-Automated/voxelizedMeshes/\"\n",
    "voxelized_mesh_files = [f for f in listdir(voxelized_mesh_path) if isfile(join(voxelized_mesh_path, f))]\n",
    "\n",
    "voxelizedFiles = []\n",
    "polycubedFiles = []\n",
    "\n",
    "for f in voxelized_mesh_files: \n",
    "    if f[-13:] == \"voxelized.txt\":\n",
    "        voxelizedFiles = np.hstack((voxelizedFiles, f))\n",
    "    \n",
    "for f in polycube_files:\n",
    "    if f[-14:] == \"finalCubes.txt\":\n",
    "        polycubedFiles = np.hstack((polycubedFiles, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 32\n",
    "voxelized_train_input, polycube_target=loadData(grid_size, polycube_path, voxelized_mesh_path, voxelizedFiles, polycubedFiles, loadFromScratch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a training and validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10 \n",
    "validation_size=50\n",
    "preprocessed_input_train, preprocessed_input_validation, preprocessed_input_train_target, preprocessed_input_validation_target = preprocessing_train(voxelized_train_input, polycube_target, validation_size, False, False)\n",
    "\n",
    "preprocessed_input_train = torch.from_numpy(preprocessed_input_train)\n",
    "preprocessed_input_validation = torch.from_numpy(preprocessed_input_validation)\n",
    "preprocessed_input_train_target = torch.from_numpy(preprocessed_input_train_target)\n",
    "preprocessed_input_validation_target = torch.from_numpy(preprocessed_input_validation_target)\n",
    "\n",
    "Ntrain = len(preprocessed_input_train[:,0,0,0,0]) \n",
    "Nvalidation = len(preprocessed_input_validation[:,0,0,0,0])\n",
    "image_size = 32\n",
    "\n",
    "train_input = np.array(preprocessed_input_train.view(Ntrain, 1,image_size, image_size, image_size))\n",
    "validation_input = np.array(preprocessed_input_validation.view(Nvalidation, 1,image_size, image_size, image_size))\n",
    "\n",
    "labels_train = np.array(preprocessed_input_train_target.view(Ntrain, 1, image_size, image_size, image_size))\n",
    "labels_validation = np.array(preprocessed_input_validation_target.view(Nvalidation, 1,image_size, image_size, image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the target \n",
    "Find the middle of the cube and the distance to the x, y and z boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input torch.Size([550, 1, 32, 32, 32])\n",
      "labels_train_cube_coords torch.Size([550, 1, 9])\n",
      "validation_input torch.Size([50, 1, 32, 32, 32])\n",
      "labels_validation_cube_coords torch.Size([50, 1, 9])\n"
     ]
    }
   ],
   "source": [
    "labels_train_cube_coords = np.zeros((len(train_input[:,0,0,0,0]), 1, 9))\n",
    "for i in range (len(train_input[:,0,0,0,0])):\n",
    "    #solution 1 - the loss is calculed based on the 9 labels \n",
    "    delta_x_left, delta_x_right, center_x, delta_y_left, delta_y_right, center_y, delta_z_left, delta_z_right, center_z = find_center_and_delta(labels_train[i, 0, :, :, :], grid_size=32)\n",
    "    labels_train_cube_coords[i,0,0] = delta_x_left\n",
    "    labels_train_cube_coords[i,0,1] = delta_x_right\n",
    "    labels_train_cube_coords[i,0,2] = center_x\n",
    "    labels_train_cube_coords[i,0,3] = delta_y_left\n",
    "    labels_train_cube_coords[i,0,4] = delta_y_right\n",
    "    labels_train_cube_coords[i,0,5] = center_y\n",
    "    labels_train_cube_coords[i,0,6] = delta_z_left\n",
    "    labels_train_cube_coords[i,0,7] = delta_z_right\n",
    "    labels_train_cube_coords[i,0,8] = center_z\n",
    "    \n",
    "labels_validation_cube_coords = np.zeros((len(labels_validation[:,0,0,0,0]), 1, 9))\n",
    "for i in range (len((validation_input[:,0,0,0,0]))):\n",
    "    #solution 1 - the loss is calculed based on the 9 labels \n",
    "    delta_x_left, delta_x_right, center_x, delta_y_left, delta_y_right, center_y, delta_z_left, delta_z_right, center_z = find_center_and_delta(labels_train[i, 0, :, :, :], grid_size=32)\n",
    "    labels_validation_cube_coords[i, 0,0] = delta_x_left\n",
    "    labels_validation_cube_coords[i, 0,1] = delta_x_right\n",
    "    labels_validation_cube_coords[i, 0,2] = center_x\n",
    "    labels_validation_cube_coords[i, 0,3] = delta_y_left\n",
    "    labels_validation_cube_coords[i, 0,4] = delta_y_right\n",
    "    labels_validation_cube_coords[i, 0,5] = center_y\n",
    "    labels_validation_cube_coords[i, 0,6] = delta_z_left\n",
    "    labels_validation_cube_coords[i, 0,7] = delta_z_right\n",
    "    labels_validation_cube_coords[i, 0,8] = center_z\n",
    "    \n",
    "    \n",
    "labels_validation_cube_coords = torch.from_numpy(labels_validation_cube_coords)\n",
    "labels_train_cube_coords = torch.from_numpy(labels_train_cube_coords)\n",
    "train_input = torch.from_numpy(train_input)\n",
    "validation_input = torch.from_numpy(validation_input)\n",
    "\n",
    "print('train_input', train_input.shape)\n",
    "print('labels_train_cube_coords', labels_train_cube_coords.shape)\n",
    "\n",
    "print('validation_input', validation_input.shape)\n",
    "print('labels_validation_cube_coords', labels_validation_cube_coords.shape)\n",
    "\n",
    "    \n",
    "    #solution 2 - the loss is calculated from the cube generated thanks to the learned coordinates \n",
    "    #output = build_cube(delta_x_left, delta_x_right, center_x, delta_y_left, delta_y_right, center_y, delta_z_left, delta_z_right, center_z, grid_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  0\n",
      "\t Training accuracy:  98.28282828282828\n",
      "\t Validation accuracy  80.44444444444444\n",
      "\t Epoch Loss  55376.265625\n",
      "Epoch Number :  1\n",
      "\t Training accuracy:  98.26262626262626\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  12929.423828125\n",
      "Epoch Number :  2\n",
      "\t Training accuracy:  98.18181818181819\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  6713.62939453125\n",
      "Epoch Number :  3\n",
      "\t Training accuracy:  98.18181818181819\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  24598.0078125\n",
      "Epoch Number :  4\n",
      "\t Training accuracy:  98.22222222222223\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  21536.29296875\n",
      "Epoch Number :  5\n",
      "\t Training accuracy:  98.28282828282828\n",
      "\t Validation accuracy  81.11111111111111\n",
      "\t Epoch Loss  9003.287109375\n",
      "Epoch Number :  6\n",
      "\t Training accuracy:  98.32323232323232\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  9528.4267578125\n",
      "Epoch Number :  7\n",
      "\t Training accuracy:  98.26262626262626\n",
      "\t Validation accuracy  80.44444444444444\n",
      "\t Epoch Loss  10330.45703125\n",
      "Epoch Number :  8\n",
      "\t Training accuracy:  98.18181818181819\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  18537.521484375\n",
      "Epoch Number :  9\n",
      "\t Training accuracy:  98.20202020202021\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  15703.181640625\n",
      "Epoch Number :  10\n",
      "\t Training accuracy:  98.18181818181819\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  20087.646484375\n",
      "Epoch Number :  11\n",
      "\t Training accuracy:  98.20202020202021\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  16189.84765625\n",
      "Epoch Number :  12\n",
      "\t Training accuracy:  98.18181818181819\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  20099.65625\n",
      "Epoch Number :  13\n",
      "\t Training accuracy:  98.20202020202021\n",
      "\t Validation accuracy  80.44444444444444\n",
      "\t Epoch Loss  8791.2822265625\n",
      "Epoch Number :  14\n",
      "\t Training accuracy:  98.26262626262626\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  4239.76708984375\n",
      "Epoch Number :  15\n",
      "\t Training accuracy:  98.20202020202021\n",
      "\t Validation accuracy  80.22222222222223\n",
      "\t Epoch Loss  2990.62548828125\n",
      "Epoch Number :  16\n",
      "\t Training accuracy:  98.18181818181819\n",
      "\t Validation accuracy  80.0\n",
      "\t Epoch Loss  1769.7042236328125\n",
      "Epoch Number :  17\n",
      "\t Training accuracy:  98.22222222222223\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  1669.98486328125\n",
      "Epoch Number :  18\n",
      "\t Training accuracy:  98.26262626262626\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  1717.471923828125\n",
      "Epoch Number :  19\n",
      "\t Training accuracy:  98.26262626262626\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  1858.0572509765625\n",
      "Epoch Number :  20\n",
      "\t Training accuracy:  98.3030303030303\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  1778.37841796875\n",
      "Epoch Number :  21\n",
      "\t Training accuracy:  98.4040404040404\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  1626.16357421875\n",
      "Epoch Number :  22\n",
      "\t Training accuracy:  98.46464646464646\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  1759.5767822265625\n",
      "Epoch Number :  23\n",
      "\t Training accuracy:  98.32323232323232\n",
      "\t Validation accuracy  80.22222222222223\n",
      "\t Epoch Loss  2592.355224609375\n",
      "Epoch Number :  24\n",
      "\t Training accuracy:  98.38383838383838\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  2928.77197265625\n",
      "Epoch Number :  25\n",
      "\t Training accuracy:  98.4040404040404\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  4202.21533203125\n",
      "Epoch Number :  26\n",
      "\t Training accuracy:  98.36363636363636\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  6615.76806640625\n",
      "Epoch Number :  27\n",
      "\t Training accuracy:  98.26262626262626\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  7796.04638671875\n",
      "Epoch Number :  28\n",
      "\t Training accuracy:  98.32323232323232\n",
      "\t Validation accuracy  80.22222222222223\n",
      "\t Epoch Loss  8219.978515625\n",
      "Epoch Number :  29\n",
      "\t Training accuracy:  98.22222222222223\n",
      "\t Validation accuracy  80.22222222222223\n",
      "\t Epoch Loss  7645.353515625\n",
      "Epoch Number :  30\n",
      "\t Training accuracy:  98.22222222222223\n",
      "\t Validation accuracy  80.44444444444444\n",
      "\t Epoch Loss  6234.837890625\n",
      "Epoch Number :  31\n",
      "\t Training accuracy:  98.28282828282828\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  4352.18408203125\n",
      "Epoch Number :  32\n",
      "\t Training accuracy:  98.28282828282828\n",
      "\t Validation accuracy  80.66666666666667\n",
      "\t Epoch Loss  2828.43017578125\n",
      "Epoch Number :  33\n",
      "\t Training accuracy:  98.22222222222223\n",
      "\t Validation accuracy  80.22222222222223\n",
      "\t Epoch Loss  2141.20751953125\n",
      "Epoch Number :  34\n",
      "\t Training accuracy:  98.20202020202021\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  2272.75537109375\n",
      "Epoch Number :  35\n",
      "\t Training accuracy:  98.26262626262626\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  2565.09912109375\n",
      "Epoch Number :  36\n",
      "\t Training accuracy:  98.32323232323232\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  2833.418212890625\n",
      "Epoch Number :  37\n",
      "\t Training accuracy:  98.22222222222223\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  2971.958984375\n",
      "Epoch Number :  38\n",
      "\t Training accuracy:  98.22222222222223\n",
      "\t Validation accuracy  80.22222222222223\n",
      "\t Epoch Loss  3106.27880859375\n",
      "Epoch Number :  39\n",
      "\t Training accuracy:  98.32323232323232\n",
      "\t Validation accuracy  80.66666666666667\n",
      "\t Epoch Loss  3968.561767578125\n",
      "Epoch Number :  40\n",
      "\t Training accuracy:  98.24242424242425\n",
      "\t Validation accuracy  80.66666666666667\n",
      "\t Epoch Loss  4755.9140625\n",
      "Epoch Number :  41\n",
      "\t Training accuracy:  98.36363636363636\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  3252.2080078125\n",
      "Epoch Number :  42\n",
      "\t Training accuracy:  98.4040404040404\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  1714.9837646484375\n",
      "Epoch Number :  43\n",
      "\t Training accuracy:  98.44444444444444\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  915.2037963867188\n",
      "Epoch Number :  44\n",
      "\t Training accuracy:  98.42424242424242\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  752.4869995117188\n",
      "Epoch Number :  45\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  731.280029296875\n",
      "Epoch Number :  46\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  710.7290649414062\n",
      "Epoch Number :  47\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  602.5029907226562\n",
      "Epoch Number :  48\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  431.5801086425781\n",
      "Epoch Number :  49\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  297.68707275390625\n",
      "Epoch Number :  50\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  240.47792053222656\n",
      "Epoch Number :  51\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  239.74188232421875\n",
      "Epoch Number :  52\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  259.0003967285156\n",
      "Epoch Number :  53\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  270.04681396484375\n",
      "Epoch Number :  54\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  262.1372985839844\n",
      "Epoch Number :  55\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  238.5911407470703\n",
      "Epoch Number :  56\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  210.3476104736328\n",
      "Epoch Number :  57\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  188.8737335205078\n",
      "Epoch Number :  58\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  183.71746826171875\n",
      "Epoch Number :  59\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  198.0760498046875\n",
      "Epoch Number :  60\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  229.50669860839844\n",
      "Epoch Number :  61\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  273.4757080078125\n",
      "Epoch Number :  62\n",
      "\t Training accuracy:  98.46464646464646\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  319.98944091796875\n",
      "Epoch Number :  63\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  367.7690124511719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  64\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  394.8517761230469\n",
      "Epoch Number :  65\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  422.125732421875\n",
      "Epoch Number :  66\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  450.8711242675781\n",
      "Epoch Number :  67\n",
      "\t Training accuracy:  98.5050505050505\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  629.8591918945312\n",
      "Epoch Number :  68\n",
      "\t Training accuracy:  98.38383838383838\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  889.16455078125\n",
      "Epoch Number :  69\n",
      "\t Training accuracy:  98.36363636363636\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  1089.98779296875\n",
      "Epoch Number :  70\n",
      "\t Training accuracy:  98.48484848484848\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  1483.556640625\n",
      "Epoch Number :  71\n",
      "\t Training accuracy:  98.42424242424242\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  1142.4835205078125\n",
      "Epoch Number :  72\n",
      "\t Training accuracy:  98.38383838383838\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  894.080078125\n",
      "Epoch Number :  73\n",
      "\t Training accuracy:  98.38383838383838\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  932.2161865234375\n",
      "Epoch Number :  74\n",
      "\t Training accuracy:  98.38383838383838\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  764.8414916992188\n",
      "Epoch Number :  75\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  727.960693359375\n",
      "Epoch Number :  76\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  546.2893676757812\n",
      "Epoch Number :  77\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  194.24676513671875\n",
      "Epoch Number :  78\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  128.42031860351562\n",
      "Epoch Number :  79\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  107.38666534423828\n",
      "Epoch Number :  80\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  97.77640533447266\n",
      "Epoch Number :  81\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  94.08182525634766\n",
      "Epoch Number :  82\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  93.98186492919922\n",
      "Epoch Number :  83\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  96.63568878173828\n",
      "Epoch Number :  84\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  101.31455993652344\n",
      "Epoch Number :  85\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  107.06324005126953\n",
      "Epoch Number :  86\n",
      "\t Training accuracy:  99.1919191919192\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  113.17533874511719\n",
      "Epoch Number :  87\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  119.60303497314453\n",
      "Epoch Number :  88\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  125.46463775634766\n",
      "Epoch Number :  89\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  128.52589416503906\n",
      "Epoch Number :  90\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  129.1809539794922\n",
      "Epoch Number :  91\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  125.73722076416016\n",
      "Epoch Number :  92\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  113.90797424316406\n",
      "Epoch Number :  93\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  105.44505310058594\n",
      "Epoch Number :  94\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  113.12554168701172\n",
      "Epoch Number :  95\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  133.2729034423828\n",
      "Epoch Number :  96\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  131.84146118164062\n",
      "Epoch Number :  97\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  137.23435974121094\n",
      "Epoch Number :  98\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  151.42218017578125\n",
      "Epoch Number :  99\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  164.5882568359375\n",
      "Epoch Number :  100\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  159.40296936035156\n",
      "Epoch Number :  101\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  153.80772399902344\n",
      "Epoch Number :  102\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  142.1759490966797\n",
      "Epoch Number :  103\n",
      "\t Training accuracy:  99.17171717171718\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  138.10476684570312\n",
      "Epoch Number :  104\n",
      "\t Training accuracy:  99.37373737373737\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  123.27227783203125\n",
      "Epoch Number :  105\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  108.20769500732422\n",
      "Epoch Number :  106\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  83.60762786865234\n",
      "Epoch Number :  107\n",
      "\t Training accuracy:  99.31313131313131\n",
      "\t Validation accuracy  87.33333333333333\n",
      "\t Epoch Loss  57.45208740234375\n",
      "Epoch Number :  108\n",
      "\t Training accuracy:  99.63636363636364\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  41.257476806640625\n",
      "Epoch Number :  109\n",
      "\t Training accuracy:  99.65656565656566\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  36.90336608886719\n",
      "Epoch Number :  110\n",
      "\t Training accuracy:  99.75757575757575\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  35.61246871948242\n",
      "Epoch Number :  111\n",
      "\t Training accuracy:  99.67676767676768\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  35.7076530456543\n",
      "Epoch Number :  112\n",
      "\t Training accuracy:  99.5959595959596\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  35.695613861083984\n",
      "Epoch Number :  113\n",
      "\t Training accuracy:  99.57575757575758\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  35.160362243652344\n",
      "Epoch Number :  114\n",
      "\t Training accuracy:  99.5959595959596\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  34.07304382324219\n",
      "Epoch Number :  115\n",
      "\t Training accuracy:  99.55555555555556\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  32.88123321533203\n",
      "Epoch Number :  116\n",
      "\t Training accuracy:  99.53535353535354\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  32.06846237182617\n",
      "Epoch Number :  117\n",
      "\t Training accuracy:  99.57575757575758\n",
      "\t Validation accuracy  87.77777777777777\n",
      "\t Epoch Loss  31.585275650024414\n",
      "Epoch Number :  118\n",
      "\t Training accuracy:  99.55555555555556\n",
      "\t Validation accuracy  87.77777777777777\n",
      "\t Epoch Loss  31.213998794555664\n",
      "Epoch Number :  119\n",
      "\t Training accuracy:  99.53535353535354\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  30.590923309326172\n",
      "Epoch Number :  120\n",
      "\t Training accuracy:  99.5959595959596\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  28.913667678833008\n",
      "Epoch Number :  121\n",
      "\t Training accuracy:  99.61616161616162\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  27.601150512695312\n",
      "Epoch Number :  122\n",
      "\t Training accuracy:  99.57575757575758\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  27.48383331298828\n",
      "Epoch Number :  123\n",
      "\t Training accuracy:  99.55555555555556\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  27.30580711364746\n",
      "Epoch Number :  124\n",
      "\t Training accuracy:  99.61616161616162\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  27.035602569580078\n",
      "Epoch Number :  125\n",
      "\t Training accuracy:  99.65656565656566\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  26.32733917236328\n",
      "Epoch Number :  126\n",
      "\t Training accuracy:  99.73737373737374\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  26.162450790405273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  127\n",
      "\t Training accuracy:  99.71717171717172\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  28.993694305419922\n",
      "Epoch Number :  128\n",
      "\t Training accuracy:  99.75757575757575\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  27.320964813232422\n",
      "Epoch Number :  129\n",
      "\t Training accuracy:  99.47474747474747\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  29.106718063354492\n",
      "Epoch Number :  130\n",
      "\t Training accuracy:  99.51515151515152\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  34.087425231933594\n",
      "Epoch Number :  131\n",
      "\t Training accuracy:  99.55555555555556\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  35.78203201293945\n",
      "Epoch Number :  132\n",
      "\t Training accuracy:  99.31313131313131\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  39.318443298339844\n",
      "Epoch Number :  133\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  39.98631286621094\n",
      "Epoch Number :  134\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  54.1899528503418\n",
      "Epoch Number :  135\n",
      "\t Training accuracy:  99.17171717171718\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  59.5263557434082\n",
      "Epoch Number :  136\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  65.606689453125\n",
      "Epoch Number :  137\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  72.60860443115234\n",
      "Epoch Number :  138\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  77.6612548828125\n",
      "Epoch Number :  139\n",
      "\t Training accuracy:  99.1919191919192\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  72.58793640136719\n",
      "Epoch Number :  140\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  79.54064178466797\n",
      "Epoch Number :  141\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  83.1946029663086\n",
      "Epoch Number :  142\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  94.40203094482422\n",
      "Epoch Number :  143\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  87.28105926513672\n",
      "Epoch Number :  144\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  104.50660705566406\n",
      "Epoch Number :  145\n",
      "\t Training accuracy:  98.64646464646465\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  111.0284194946289\n",
      "Epoch Number :  146\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  112.26744079589844\n",
      "Epoch Number :  147\n",
      "\t Training accuracy:  98.88888888888889\n",
      "\t Validation accuracy  80.88888888888889\n",
      "\t Epoch Loss  133.73899841308594\n",
      "Epoch Number :  148\n",
      "\t Training accuracy:  98.4040404040404\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  161.514404296875\n",
      "Epoch Number :  149\n",
      "\t Training accuracy:  98.5050505050505\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  181.95359802246094\n",
      "Epoch Number :  150\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  146.03990173339844\n",
      "Epoch Number :  151\n",
      "\t Training accuracy:  98.96969696969697\n",
      "\t Validation accuracy  80.22222222222223\n",
      "\t Epoch Loss  169.93641662597656\n",
      "Epoch Number :  152\n",
      "\t Training accuracy:  98.62626262626263\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  176.95042419433594\n",
      "Epoch Number :  153\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  198.72659301757812\n",
      "Epoch Number :  154\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  205.72146606445312\n",
      "Epoch Number :  155\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  80.66666666666667\n",
      "\t Epoch Loss  189.13514709472656\n",
      "Epoch Number :  156\n",
      "\t Training accuracy:  98.28282828282828\n",
      "\t Validation accuracy  81.33333333333333\n",
      "\t Epoch Loss  202.64154052734375\n",
      "Epoch Number :  157\n",
      "\t Training accuracy:  98.70707070707071\n",
      "\t Validation accuracy  81.55555555555556\n",
      "\t Epoch Loss  191.04916381835938\n",
      "Epoch Number :  158\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  181.2809295654297\n",
      "Epoch Number :  159\n",
      "\t Training accuracy:  98.34343434343434\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  198.49903869628906\n",
      "Epoch Number :  160\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  186.03814697265625\n",
      "Epoch Number :  161\n",
      "\t Training accuracy:  98.54545454545455\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  176.36480712890625\n",
      "Epoch Number :  162\n",
      "\t Training accuracy:  98.66666666666667\n",
      "\t Validation accuracy  86.66666666666667\n",
      "\t Epoch Loss  166.87338256835938\n",
      "Epoch Number :  163\n",
      "\t Training accuracy:  98.58585858585859\n",
      "\t Validation accuracy  87.11111111111111\n",
      "\t Epoch Loss  137.80503845214844\n",
      "Epoch Number :  164\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  87.03001403808594\n",
      "Epoch Number :  165\n",
      "\t Training accuracy:  99.27272727272727\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  59.78756332397461\n",
      "Epoch Number :  166\n",
      "\t Training accuracy:  99.23232323232324\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  44.20028305053711\n",
      "Epoch Number :  167\n",
      "\t Training accuracy:  99.4949494949495\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  38.019832611083984\n",
      "Epoch Number :  168\n",
      "\t Training accuracy:  99.73737373737374\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  31.451330184936523\n",
      "Epoch Number :  169\n",
      "\t Training accuracy:  99.75757575757575\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  25.270931243896484\n",
      "Epoch Number :  170\n",
      "\t Training accuracy:  99.75757575757575\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  20.738143920898438\n",
      "Epoch Number :  171\n",
      "\t Training accuracy:  99.79797979797979\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  17.400859832763672\n",
      "Epoch Number :  172\n",
      "\t Training accuracy:  99.8989898989899\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  15.183870315551758\n",
      "Epoch Number :  173\n",
      "\t Training accuracy:  99.95959595959596\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  13.639449119567871\n",
      "Epoch Number :  174\n",
      "\t Training accuracy:  99.93939393939394\n",
      "\t Validation accuracy  86.44444444444444\n",
      "\t Epoch Loss  12.173945426940918\n",
      "Epoch Number :  175\n",
      "\t Training accuracy:  99.93939393939394\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  10.932830810546875\n",
      "Epoch Number :  176\n",
      "\t Training accuracy:  99.95959595959596\n",
      "\t Validation accuracy  86.88888888888889\n",
      "\t Epoch Loss  10.141387939453125\n",
      "Epoch Number :  177\n",
      "\t Training accuracy:  99.95959595959596\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  9.827431678771973\n",
      "Epoch Number :  178\n",
      "\t Training accuracy:  99.93939393939394\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  9.911697387695312\n",
      "Epoch Number :  179\n",
      "\t Training accuracy:  99.83838383838383\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  10.382554054260254\n",
      "Epoch Number :  180\n",
      "\t Training accuracy:  99.75757575757575\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  11.288311004638672\n",
      "Epoch Number :  181\n",
      "\t Training accuracy:  99.75757575757575\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  12.439026832580566\n",
      "Epoch Number :  182\n",
      "\t Training accuracy:  99.77777777777777\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  12.034513473510742\n",
      "Epoch Number :  183\n",
      "\t Training accuracy:  99.65656565656566\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  14.486004829406738\n",
      "Epoch Number :  184\n",
      "\t Training accuracy:  99.71717171717172\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  15.19658374786377\n",
      "Epoch Number :  185\n",
      "\t Training accuracy:  99.57575757575758\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  16.377466201782227\n",
      "Epoch Number :  186\n",
      "\t Training accuracy:  99.81818181818181\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  15.591011047363281\n",
      "Epoch Number :  187\n",
      "\t Training accuracy:  99.79797979797979\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  14.990045547485352\n",
      "Epoch Number :  188\n",
      "\t Training accuracy:  99.8989898989899\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  14.989615440368652\n",
      "Epoch Number :  189\n",
      "\t Training accuracy:  99.6969696969697\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  20.29137420654297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  190\n",
      "\t Training accuracy:  99.85858585858585\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  25.74904441833496\n",
      "Epoch Number :  191\n",
      "\t Training accuracy:  99.1919191919192\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  23.24734878540039\n",
      "Epoch Number :  192\n",
      "\t Training accuracy:  99.83838383838383\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  25.879850387573242\n",
      "Epoch Number :  193\n",
      "\t Training accuracy:  99.47474747474747\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  26.992956161499023\n",
      "Epoch Number :  194\n",
      "\t Training accuracy:  99.27272727272727\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  33.75056076049805\n",
      "Epoch Number :  195\n",
      "\t Training accuracy:  99.5959595959596\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  30.32574462890625\n",
      "Epoch Number :  196\n",
      "\t Training accuracy:  99.25252525252525\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  26.401893615722656\n",
      "Epoch Number :  197\n",
      "\t Training accuracy:  99.23232323232324\n",
      "\t Validation accuracy  81.77777777777777\n",
      "\t Epoch Loss  28.673799514770508\n",
      "Epoch Number :  198\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  25.884366989135742\n",
      "Epoch Number :  199\n",
      "\t Training accuracy:  99.4949494949495\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  36.8276252746582\n",
      "Epoch Number :  200\n",
      "\t Training accuracy:  99.47474747474747\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  61.645965576171875\n",
      "Epoch Number :  201\n",
      "\t Training accuracy:  99.31313131313131\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  51.889400482177734\n",
      "Epoch Number :  202\n",
      "\t Training accuracy:  99.63636363636364\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  64.20638275146484\n",
      "Epoch Number :  203\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  46.25017166137695\n",
      "Epoch Number :  204\n",
      "\t Training accuracy:  99.31313131313131\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  42.2198600769043\n",
      "Epoch Number :  205\n",
      "\t Training accuracy:  99.33333333333333\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  47.31656265258789\n",
      "Epoch Number :  206\n",
      "\t Training accuracy:  99.27272727272727\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  59.783382415771484\n",
      "Epoch Number :  207\n",
      "\t Training accuracy:  99.25252525252525\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  68.86141204833984\n",
      "Epoch Number :  208\n",
      "\t Training accuracy:  99.43434343434343\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  58.512428283691406\n",
      "Epoch Number :  209\n",
      "\t Training accuracy:  99.27272727272727\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  61.07435989379883\n",
      "Epoch Number :  210\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  63.00127029418945\n",
      "Epoch Number :  211\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  54.44202423095703\n",
      "Epoch Number :  212\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  60.01847839355469\n",
      "Epoch Number :  213\n",
      "\t Training accuracy:  99.27272727272727\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  49.387264251708984\n",
      "Epoch Number :  214\n",
      "\t Training accuracy:  98.86868686868686\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  50.12915802001953\n",
      "Epoch Number :  215\n",
      "\t Training accuracy:  98.9090909090909\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  45.51789474487305\n",
      "Epoch Number :  216\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  45.701210021972656\n",
      "Epoch Number :  217\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  45.097347259521484\n",
      "Epoch Number :  218\n",
      "\t Training accuracy:  99.41414141414141\n",
      "\t Validation accuracy  86.66666666666667\n",
      "\t Epoch Loss  46.22551727294922\n",
      "Epoch Number :  219\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  43.9013786315918\n",
      "Epoch Number :  220\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  42.69141387939453\n",
      "Epoch Number :  221\n",
      "\t Training accuracy:  99.21212121212122\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  43.5196533203125\n",
      "Epoch Number :  222\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  38.22923278808594\n",
      "Epoch Number :  223\n",
      "\t Training accuracy:  99.35353535353535\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  42.76335525512695\n",
      "Epoch Number :  224\n",
      "\t Training accuracy:  99.29292929292929\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  44.77907180786133\n",
      "Epoch Number :  225\n",
      "\t Training accuracy:  99.01010101010101\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  40.91411590576172\n",
      "Epoch Number :  226\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  44.966732025146484\n",
      "Epoch Number :  227\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  41.20044708251953\n",
      "Epoch Number :  228\n",
      "\t Training accuracy:  99.31313131313131\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  47.74412155151367\n",
      "Epoch Number :  229\n",
      "\t Training accuracy:  98.72727272727273\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  52.595611572265625\n",
      "Epoch Number :  230\n",
      "\t Training accuracy:  98.86868686868686\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  42.33516311645508\n",
      "Epoch Number :  231\n",
      "\t Training accuracy:  99.27272727272727\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  41.33177947998047\n",
      "Epoch Number :  232\n",
      "\t Training accuracy:  99.17171717171718\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  33.933448791503906\n",
      "Epoch Number :  233\n",
      "\t Training accuracy:  99.43434343434343\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  30.294570922851562\n",
      "Epoch Number :  234\n",
      "\t Training accuracy:  99.53535353535354\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  20.720109939575195\n",
      "Epoch Number :  235\n",
      "\t Training accuracy:  99.71717171717172\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  15.291723251342773\n",
      "Epoch Number :  236\n",
      "\t Training accuracy:  99.4949494949495\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  12.212296485900879\n",
      "Epoch Number :  237\n",
      "\t Training accuracy:  99.73737373737374\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  11.407327651977539\n",
      "Epoch Number :  238\n",
      "\t Training accuracy:  99.91919191919192\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  10.741854667663574\n",
      "Epoch Number :  239\n",
      "\t Training accuracy:  99.75757575757575\n",
      "\t Validation accuracy  86.66666666666667\n",
      "\t Epoch Loss  12.081560134887695\n",
      "Epoch Number :  240\n",
      "\t Training accuracy:  99.5959595959596\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  12.793619155883789\n",
      "Epoch Number :  241\n",
      "\t Training accuracy:  99.87878787878788\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  19.120296478271484\n",
      "Epoch Number :  242\n",
      "\t Training accuracy:  99.67676767676768\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  19.770204544067383\n",
      "Epoch Number :  243\n",
      "\t Training accuracy:  99.65656565656566\n",
      "\t Validation accuracy  86.44444444444444\n",
      "\t Epoch Loss  19.902008056640625\n",
      "Epoch Number :  244\n",
      "\t Training accuracy:  99.51515151515152\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  22.427152633666992\n",
      "Epoch Number :  245\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  22.32209587097168\n",
      "Epoch Number :  246\n",
      "\t Training accuracy:  99.75757575757575\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  19.582937240600586\n",
      "Epoch Number :  247\n",
      "\t Training accuracy:  99.55555555555556\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  24.294462203979492\n",
      "Epoch Number :  248\n",
      "\t Training accuracy:  99.29292929292929\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  21.862686157226562\n",
      "Epoch Number :  249\n",
      "\t Training accuracy:  99.35353535353535\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  23.38109588623047\n",
      "Epoch Number :  250\n",
      "\t Training accuracy:  99.17171717171718\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  26.382274627685547\n",
      "Epoch Number :  251\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  32.36936950683594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  252\n",
      "\t Training accuracy:  99.37373737373737\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  29.092912673950195\n",
      "Epoch Number :  253\n",
      "\t Training accuracy:  99.63636363636364\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  23.389135360717773\n",
      "Epoch Number :  254\n",
      "\t Training accuracy:  99.39393939393939\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  24.260356903076172\n",
      "Epoch Number :  255\n",
      "\t Training accuracy:  99.27272727272727\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  37.133365631103516\n",
      "Epoch Number :  256\n",
      "\t Training accuracy:  99.5959595959596\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  42.649234771728516\n",
      "Epoch Number :  257\n",
      "\t Training accuracy:  99.47474747474747\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  34.64773941040039\n",
      "Epoch Number :  258\n",
      "\t Training accuracy:  99.4949494949495\n",
      "\t Validation accuracy  87.11111111111111\n",
      "\t Epoch Loss  25.171707153320312\n",
      "Epoch Number :  259\n",
      "\t Training accuracy:  99.31313131313131\n",
      "\t Validation accuracy  87.11111111111111\n",
      "\t Epoch Loss  25.655502319335938\n",
      "Epoch Number :  260\n",
      "\t Training accuracy:  99.41414141414141\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  20.77437973022461\n",
      "Epoch Number :  261\n",
      "\t Training accuracy:  99.1919191919192\n",
      "\t Validation accuracy  86.66666666666667\n",
      "\t Epoch Loss  26.4816837310791\n",
      "Epoch Number :  262\n",
      "\t Training accuracy:  99.5959595959596\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  27.820098876953125\n",
      "Epoch Number :  263\n",
      "\t Training accuracy:  99.21212121212122\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  26.80038070678711\n",
      "Epoch Number :  264\n",
      "\t Training accuracy:  99.39393939393939\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  29.920812606811523\n",
      "Epoch Number :  265\n",
      "\t Training accuracy:  99.29292929292929\n",
      "\t Validation accuracy  87.33333333333333\n",
      "\t Epoch Loss  24.468067169189453\n",
      "Epoch Number :  266\n",
      "\t Training accuracy:  99.43434343434343\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  21.808332443237305\n",
      "Epoch Number :  267\n",
      "\t Training accuracy:  99.31313131313131\n",
      "\t Validation accuracy  86.44444444444444\n",
      "\t Epoch Loss  21.216291427612305\n",
      "Epoch Number :  268\n",
      "\t Training accuracy:  99.45454545454545\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  21.127079010009766\n",
      "Epoch Number :  269\n",
      "\t Training accuracy:  99.4949494949495\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  18.580114364624023\n",
      "Epoch Number :  270\n",
      "\t Training accuracy:  99.33333333333333\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  19.057071685791016\n",
      "Epoch Number :  271\n",
      "\t Training accuracy:  99.39393939393939\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  23.028282165527344\n",
      "Epoch Number :  272\n",
      "\t Training accuracy:  99.35353535353535\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  23.76520538330078\n",
      "Epoch Number :  273\n",
      "\t Training accuracy:  99.43434343434343\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  27.158662796020508\n",
      "Epoch Number :  274\n",
      "\t Training accuracy:  99.57575757575758\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  28.605558395385742\n",
      "Epoch Number :  275\n",
      "\t Training accuracy:  99.37373737373737\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  28.890188217163086\n",
      "Epoch Number :  276\n",
      "\t Training accuracy:  99.61616161616162\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  26.74601936340332\n",
      "Epoch Number :  277\n",
      "\t Training accuracy:  99.57575757575758\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  30.13909149169922\n",
      "Epoch Number :  278\n",
      "\t Training accuracy:  99.53535353535354\n",
      "\t Validation accuracy  86.44444444444444\n",
      "\t Epoch Loss  30.17816734313965\n",
      "Epoch Number :  279\n",
      "\t Training accuracy:  99.35353535353535\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  23.08302879333496\n",
      "Epoch Number :  280\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  30.81277084350586\n",
      "Epoch Number :  281\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  40.759033203125\n",
      "Epoch Number :  282\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  55.16892623901367\n",
      "Epoch Number :  283\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  86.44444444444444\n",
      "\t Epoch Loss  56.70494079589844\n",
      "Epoch Number :  284\n",
      "\t Training accuracy:  99.29292929292929\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  49.94060134887695\n",
      "Epoch Number :  285\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  44.79240036010742\n",
      "Epoch Number :  286\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  48.91181945800781\n",
      "Epoch Number :  287\n",
      "\t Training accuracy:  99.17171717171718\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  56.34352111816406\n",
      "Epoch Number :  288\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  65.1881332397461\n",
      "Epoch Number :  289\n",
      "\t Training accuracy:  99.23232323232324\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  46.65861511230469\n",
      "Epoch Number :  290\n",
      "\t Training accuracy:  99.37373737373737\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  33.110137939453125\n",
      "Epoch Number :  291\n",
      "\t Training accuracy:  99.31313131313131\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  39.615604400634766\n",
      "Epoch Number :  292\n",
      "\t Training accuracy:  99.1919191919192\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  40.64336395263672\n",
      "Epoch Number :  293\n",
      "\t Training accuracy:  99.25252525252525\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  31.883821487426758\n",
      "Epoch Number :  294\n",
      "\t Training accuracy:  99.55555555555556\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  37.423343658447266\n",
      "Epoch Number :  295\n",
      "\t Training accuracy:  99.4949494949495\n",
      "\t Validation accuracy  87.11111111111111\n",
      "\t Epoch Loss  30.634689331054688\n",
      "Epoch Number :  296\n",
      "\t Training accuracy:  99.61616161616162\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  25.319684982299805\n",
      "Epoch Number :  297\n",
      "\t Training accuracy:  99.47474747474747\n",
      "\t Validation accuracy  86.66666666666667\n",
      "\t Epoch Loss  27.513071060180664\n",
      "Epoch Number :  298\n",
      "\t Training accuracy:  99.43434343434343\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  22.06905746459961\n",
      "Epoch Number :  299\n",
      "\t Training accuracy:  99.71717171717172\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  20.09676742553711\n",
      "Epoch Number :  300\n",
      "\t Training accuracy:  99.35353535353535\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  17.27147102355957\n",
      "Epoch Number :  301\n",
      "\t Training accuracy:  99.1919191919192\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  21.182872772216797\n",
      "Epoch Number :  302\n",
      "\t Training accuracy:  99.31313131313131\n",
      "\t Validation accuracy  86.66666666666667\n",
      "\t Epoch Loss  23.836172103881836\n",
      "Epoch Number :  303\n",
      "\t Training accuracy:  99.51515151515152\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  23.390888214111328\n",
      "Epoch Number :  304\n",
      "\t Training accuracy:  99.27272727272727\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  25.612634658813477\n",
      "Epoch Number :  305\n",
      "\t Training accuracy:  99.43434343434343\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  28.824024200439453\n",
      "Epoch Number :  306\n",
      "\t Training accuracy:  99.45454545454545\n",
      "\t Validation accuracy  88.0\n",
      "\t Epoch Loss  27.169633865356445\n",
      "Epoch Number :  307\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  23.763883590698242\n",
      "Epoch Number :  308\n",
      "\t Training accuracy:  99.33333333333333\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  30.020999908447266\n",
      "Epoch Number :  309\n",
      "\t Training accuracy:  99.41414141414141\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  33.86186981201172\n",
      "Epoch Number :  310\n",
      "\t Training accuracy:  99.27272727272727\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  22.24584197998047\n",
      "Epoch Number :  311\n",
      "\t Training accuracy:  99.33333333333333\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  31.4984073638916\n",
      "Epoch Number :  312\n",
      "\t Training accuracy:  98.96969696969697\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  33.179264068603516\n",
      "Epoch Number :  313\n",
      "\t Training accuracy:  99.23232323232324\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  35.0822639465332\n",
      "Epoch Number :  314\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  25.583782196044922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  315\n",
      "\t Training accuracy:  99.39393939393939\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  28.30374526977539\n",
      "Epoch Number :  316\n",
      "\t Training accuracy:  99.55555555555556\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  30.365013122558594\n",
      "Epoch Number :  317\n",
      "\t Training accuracy:  99.35353535353535\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  30.435516357421875\n",
      "Epoch Number :  318\n",
      "\t Training accuracy:  99.41414141414141\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  21.596311569213867\n",
      "Epoch Number :  319\n",
      "\t Training accuracy:  99.57575757575758\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  26.301040649414062\n",
      "Epoch Number :  320\n",
      "\t Training accuracy:  98.96969696969697\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  27.75873565673828\n",
      "Epoch Number :  321\n",
      "\t Training accuracy:  98.84848484848484\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  31.012319564819336\n",
      "Epoch Number :  322\n",
      "\t Training accuracy:  99.37373737373737\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  32.80887985229492\n",
      "Epoch Number :  323\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  30.884899139404297\n",
      "Epoch Number :  324\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  31.17341423034668\n",
      "Epoch Number :  325\n",
      "\t Training accuracy:  99.35353535353535\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  29.004072189331055\n",
      "Epoch Number :  326\n",
      "\t Training accuracy:  99.13131313131314\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  26.315906524658203\n",
      "Epoch Number :  327\n",
      "\t Training accuracy:  99.31313131313131\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  32.96349334716797\n",
      "Epoch Number :  328\n",
      "\t Training accuracy:  99.43434343434343\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  32.736473083496094\n",
      "Epoch Number :  329\n",
      "\t Training accuracy:  99.37373737373737\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  28.55413818359375\n",
      "Epoch Number :  330\n",
      "\t Training accuracy:  99.21212121212122\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  30.88006019592285\n",
      "Epoch Number :  331\n",
      "\t Training accuracy:  99.47474747474747\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  26.645793914794922\n",
      "Epoch Number :  332\n",
      "\t Training accuracy:  99.55555555555556\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  30.127458572387695\n",
      "Epoch Number :  333\n",
      "\t Training accuracy:  99.53535353535354\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  34.34151840209961\n",
      "Epoch Number :  334\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  34.338401794433594\n",
      "Epoch Number :  335\n",
      "\t Training accuracy:  99.25252525252525\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  46.67664337158203\n",
      "Epoch Number :  336\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  59.24077606201172\n",
      "Epoch Number :  337\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  57.45387649536133\n",
      "Epoch Number :  338\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  46.34984588623047\n",
      "Epoch Number :  339\n",
      "\t Training accuracy:  98.56565656565657\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  43.932281494140625\n",
      "Epoch Number :  340\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  65.95606231689453\n",
      "Epoch Number :  341\n",
      "\t Training accuracy:  99.29292929292929\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  83.2980728149414\n",
      "Epoch Number :  342\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  60.306549072265625\n",
      "Epoch Number :  343\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  66.08379364013672\n",
      "Epoch Number :  344\n",
      "\t Training accuracy:  99.25252525252525\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  38.576847076416016\n",
      "Epoch Number :  345\n",
      "\t Training accuracy:  99.13131313131314\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  51.52693557739258\n",
      "Epoch Number :  346\n",
      "\t Training accuracy:  99.27272727272727\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  57.74250411987305\n",
      "Epoch Number :  347\n",
      "\t Training accuracy:  99.61616161616162\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  55.46344757080078\n",
      "Epoch Number :  348\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  64.19327545166016\n",
      "Epoch Number :  349\n",
      "\t Training accuracy:  99.25252525252525\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  70.61914825439453\n",
      "Epoch Number :  350\n",
      "\t Training accuracy:  99.37373737373737\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  61.1711540222168\n",
      "Epoch Number :  351\n",
      "\t Training accuracy:  99.13131313131314\n",
      "\t Validation accuracy  85.77777777777777\n",
      "\t Epoch Loss  49.532379150390625\n",
      "Epoch Number :  352\n",
      "\t Training accuracy:  99.35353535353535\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  61.736148834228516\n",
      "Epoch Number :  353\n",
      "\t Training accuracy:  99.01010101010101\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  71.85769653320312\n",
      "Epoch Number :  354\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  87.11111111111111\n",
      "\t Epoch Loss  48.184478759765625\n",
      "Epoch Number :  355\n",
      "\t Training accuracy:  99.25252525252525\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  64.05611419677734\n",
      "Epoch Number :  356\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  71.56928253173828\n",
      "Epoch Number :  357\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  76.47620391845703\n",
      "Epoch Number :  358\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  62.70585250854492\n",
      "Epoch Number :  359\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  75.8320083618164\n",
      "Epoch Number :  360\n",
      "\t Training accuracy:  98.76767676767676\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  59.132408142089844\n",
      "Epoch Number :  361\n",
      "\t Training accuracy:  98.8080808080808\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  60.87208557128906\n",
      "Epoch Number :  362\n",
      "\t Training accuracy:  98.9090909090909\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  69.05277252197266\n",
      "Epoch Number :  363\n",
      "\t Training accuracy:  99.1919191919192\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  68.06048583984375\n",
      "Epoch Number :  364\n",
      "\t Training accuracy:  99.21212121212122\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  65.0870361328125\n",
      "Epoch Number :  365\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  50.71391677856445\n",
      "Epoch Number :  366\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  28.061704635620117\n",
      "Epoch Number :  367\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  86.88888888888889\n",
      "\t Epoch Loss  29.075664520263672\n",
      "Epoch Number :  368\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  42.31428909301758\n",
      "Epoch Number :  369\n",
      "\t Training accuracy:  99.39393939393939\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  43.116966247558594\n",
      "Epoch Number :  370\n",
      "\t Training accuracy:  98.92929292929293\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  44.665157318115234\n",
      "Epoch Number :  371\n",
      "\t Training accuracy:  99.01010101010101\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  41.052005767822266\n",
      "Epoch Number :  372\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  42.59774398803711\n",
      "Epoch Number :  373\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  44.45112991333008\n",
      "Epoch Number :  374\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  42.05727005004883\n",
      "Epoch Number :  375\n",
      "\t Training accuracy:  99.23232323232324\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  55.0695686340332\n",
      "Epoch Number :  376\n",
      "\t Training accuracy:  99.37373737373737\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  52.1273307800293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  377\n",
      "\t Training accuracy:  99.23232323232324\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  50.03337097167969\n",
      "Epoch Number :  378\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  86.88888888888889\n",
      "\t Epoch Loss  62.6310920715332\n",
      "Epoch Number :  379\n",
      "\t Training accuracy:  99.17171717171718\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  51.94892120361328\n",
      "Epoch Number :  380\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  58.14137268066406\n",
      "Epoch Number :  381\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  87.2336654663086\n",
      "Epoch Number :  382\n",
      "\t Training accuracy:  99.01010101010101\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  41.78303909301758\n",
      "Epoch Number :  383\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  34.62181854248047\n",
      "Epoch Number :  384\n",
      "\t Training accuracy:  99.17171717171718\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  52.72601318359375\n",
      "Epoch Number :  385\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  59.68378829956055\n",
      "Epoch Number :  386\n",
      "\t Training accuracy:  99.05050505050505\n",
      "\t Validation accuracy  83.33333333333333\n",
      "\t Epoch Loss  64.10671997070312\n",
      "Epoch Number :  387\n",
      "\t Training accuracy:  98.94949494949495\n",
      "\t Validation accuracy  82.44444444444444\n",
      "\t Epoch Loss  75.22042083740234\n",
      "Epoch Number :  388\n",
      "\t Training accuracy:  98.9090909090909\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  77.80359649658203\n",
      "Epoch Number :  389\n",
      "\t Training accuracy:  98.74747474747475\n",
      "\t Validation accuracy  82.66666666666667\n",
      "\t Epoch Loss  104.30155181884766\n",
      "Epoch Number :  390\n",
      "\t Training accuracy:  98.60606060606061\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  107.88444519042969\n",
      "Epoch Number :  391\n",
      "\t Training accuracy:  98.86868686868686\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  84.41212463378906\n",
      "Epoch Number :  392\n",
      "\t Training accuracy:  99.23232323232324\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  73.27758026123047\n",
      "Epoch Number :  393\n",
      "\t Training accuracy:  98.68686868686869\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  56.21845626831055\n",
      "Epoch Number :  394\n",
      "\t Training accuracy:  99.11111111111111\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  53.218711853027344\n",
      "Epoch Number :  395\n",
      "\t Training accuracy:  98.98989898989899\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  66.294921875\n",
      "Epoch Number :  396\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  84.22222222222223\n",
      "\t Epoch Loss  64.64567565917969\n",
      "Epoch Number :  397\n",
      "\t Training accuracy:  98.78787878787878\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  65.07785034179688\n",
      "Epoch Number :  398\n",
      "\t Training accuracy:  99.27272727272727\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  49.461944580078125\n",
      "Epoch Number :  399\n",
      "\t Training accuracy:  99.41414141414141\n",
      "\t Validation accuracy  86.44444444444444\n",
      "\t Epoch Loss  30.047531127929688\n",
      "Epoch Number :  400\n",
      "\t Training accuracy:  99.35353535353535\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  42.96002960205078\n",
      "Epoch Number :  401\n",
      "\t Training accuracy:  99.13131313131314\n",
      "\t Validation accuracy  84.66666666666667\n",
      "\t Epoch Loss  43.1391487121582\n",
      "Epoch Number :  402\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  44.89458084106445\n",
      "Epoch Number :  403\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  83.55555555555556\n",
      "\t Epoch Loss  35.853763580322266\n",
      "Epoch Number :  404\n",
      "\t Training accuracy:  99.55555555555556\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  30.129676818847656\n",
      "Epoch Number :  405\n",
      "\t Training accuracy:  99.33333333333333\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  33.716529846191406\n",
      "Epoch Number :  406\n",
      "\t Training accuracy:  99.1919191919192\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  45.337745666503906\n",
      "Epoch Number :  407\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  87.33333333333333\n",
      "\t Epoch Loss  56.34341049194336\n",
      "Epoch Number :  408\n",
      "\t Training accuracy:  99.03030303030303\n",
      "\t Validation accuracy  84.0\n",
      "\t Epoch Loss  47.11351776123047\n",
      "Epoch Number :  409\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  45.232940673828125\n",
      "Epoch Number :  410\n",
      "\t Training accuracy:  99.31313131313131\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  54.945499420166016\n",
      "Epoch Number :  411\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  29.19510269165039\n",
      "Epoch Number :  412\n",
      "\t Training accuracy:  99.43434343434343\n",
      "\t Validation accuracy  87.11111111111111\n",
      "\t Epoch Loss  32.91545104980469\n",
      "Epoch Number :  413\n",
      "\t Training accuracy:  99.75757575757575\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  29.039846420288086\n",
      "Epoch Number :  414\n",
      "\t Training accuracy:  99.51515151515152\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  21.172138214111328\n",
      "Epoch Number :  415\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  35.70549774169922\n",
      "Epoch Number :  416\n",
      "\t Training accuracy:  99.57575757575758\n",
      "\t Validation accuracy  86.44444444444444\n",
      "\t Epoch Loss  37.1439323425293\n",
      "Epoch Number :  417\n",
      "\t Training accuracy:  99.37373737373737\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  30.87308120727539\n",
      "Epoch Number :  418\n",
      "\t Training accuracy:  99.67676767676768\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  29.645124435424805\n",
      "Epoch Number :  419\n",
      "\t Training accuracy:  99.53535353535354\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  20.730323791503906\n",
      "Epoch Number :  420\n",
      "\t Training accuracy:  99.51515151515152\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  21.46227264404297\n",
      "Epoch Number :  421\n",
      "\t Training accuracy:  99.55555555555556\n",
      "\t Validation accuracy  86.44444444444444\n",
      "\t Epoch Loss  38.79313659667969\n",
      "Epoch Number :  422\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  82.0\n",
      "\t Epoch Loss  25.584609985351562\n",
      "Epoch Number :  423\n",
      "\t Training accuracy:  99.23232323232324\n",
      "\t Validation accuracy  84.44444444444444\n",
      "\t Epoch Loss  31.67287254333496\n",
      "Epoch Number :  424\n",
      "\t Training accuracy:  99.39393939393939\n",
      "\t Validation accuracy  85.11111111111111\n",
      "\t Epoch Loss  33.386016845703125\n",
      "Epoch Number :  425\n",
      "\t Training accuracy:  99.1919191919192\n",
      "\t Validation accuracy  82.22222222222223\n",
      "\t Epoch Loss  23.691356658935547\n",
      "Epoch Number :  426\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  86.22222222222223\n",
      "\t Epoch Loss  32.97244644165039\n",
      "Epoch Number :  427\n",
      "\t Training accuracy:  99.25252525252525\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  36.87456512451172\n",
      "Epoch Number :  428\n",
      "\t Training accuracy:  99.41414141414141\n",
      "\t Validation accuracy  82.88888888888889\n",
      "\t Epoch Loss  34.87487030029297\n",
      "Epoch Number :  429\n",
      "\t Training accuracy:  99.0909090909091\n",
      "\t Validation accuracy  83.11111111111111\n",
      "\t Epoch Loss  36.73583221435547\n",
      "Epoch Number :  430\n",
      "\t Training accuracy:  99.61616161616162\n",
      "\t Validation accuracy  84.88888888888889\n",
      "\t Epoch Loss  25.467702865600586\n",
      "Epoch Number :  431\n",
      "\t Training accuracy:  99.15151515151516\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  21.0904541015625\n",
      "Epoch Number :  432\n",
      "\t Training accuracy:  98.82828282828282\n",
      "\t Validation accuracy  86.44444444444444\n",
      "\t Epoch Loss  27.66119956970215\n",
      "Epoch Number :  433\n",
      "\t Training accuracy:  99.4949494949495\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  19.352386474609375\n",
      "Epoch Number :  434\n",
      "\t Training accuracy:  99.33333333333333\n",
      "\t Validation accuracy  85.33333333333333\n",
      "\t Epoch Loss  21.715547561645508\n",
      "Epoch Number :  435\n",
      "\t Training accuracy:  99.63636363636364\n",
      "\t Validation accuracy  86.44444444444444\n",
      "\t Epoch Loss  26.44713592529297\n",
      "Epoch Number :  436\n",
      "\t Training accuracy:  99.07070707070707\n",
      "\t Validation accuracy  83.77777777777777\n",
      "\t Epoch Loss  28.126529693603516\n",
      "Epoch Number :  437\n",
      "\t Training accuracy:  99.41414141414141\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  41.678104400634766\n",
      "Epoch Number :  438\n",
      "\t Training accuracy:  99.45454545454545\n",
      "\t Validation accuracy  85.55555555555556\n",
      "\t Epoch Loss  33.97068786621094\n",
      "Epoch Number :  439\n",
      "\t Training accuracy:  99.5959595959596\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  19.276674270629883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  440\n",
      "\t Training accuracy:  99.79797979797979\n",
      "\t Validation accuracy  87.11111111111111\n",
      "\t Epoch Loss  15.485883712768555\n",
      "Epoch Number :  441\n",
      "\t Training accuracy:  99.63636363636364\n",
      "\t Validation accuracy  88.88888888888889\n",
      "\t Epoch Loss  11.840843200683594\n",
      "Epoch Number :  442\n",
      "\t Training accuracy:  99.31313131313131\n",
      "\t Validation accuracy  86.0\n",
      "\t Epoch Loss  19.921611785888672\n"
     ]
    }
   ],
   "source": [
    "# Train network \n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.PoissonNLLLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.SmoothL1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_input = Variable(train_input).float()\n",
    "validation_input = Variable(validation_input).float()\n",
    "\n",
    "if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "    train_target = Variable(labels_train_cube_coords)  # keep long tensors\n",
    "    validation_target = Variable(labels_validation_cube_coords, \n",
    "    uires_grad=False) # convert to float\n",
    "    Noutputs = 18\n",
    "    \n",
    "elif isinstance(criterion, nn.NLLLoss):\n",
    "    train_target = Variable(labels_train_cube_coords)  # keep long tensors\n",
    "    validation_target = Variable(labels_validation_cube_coords, requires_grad=False) # convert to float\n",
    "    Noutputs = 18\n",
    "    \n",
    "else:\n",
    "    train_target = Variable(labels_train_cube_coords, requires_grad=False).float() # convert to float\n",
    "    validation_target = Variable(labels_validation_cube_coords, requires_grad=False).float() # convert to float\n",
    "    Noutputs = 9\n",
    "\n",
    "Nbatches = int(math.ceil(Ntrain/batch_size))\n",
    "Nepochs = 500\n",
    "#seeds = list(range(15)) #Test 15 different seeds but always the seeds from 0 to 15 so that the weights are always initialized in a reproducible way\n",
    "#Nrep = len(seeds)\n",
    "Nrep = 1\n",
    "\n",
    "train_errors = torch.Tensor(Nrep, Nepochs).zero_()\n",
    "test_errors = torch.Tensor(Nrep, Nepochs).zero_()\n",
    "validation_errors = torch.Tensor(Nrep, Nepochs).zero_()\n",
    "ep_loss = torch.Tensor(Nrep, Nepochs).zero_()\n",
    "\n",
    "for i_rep in range(Nrep):    \n",
    "    #print('Repetition', seeds[i_rep])\n",
    "    #torch.manual_seed(seeds[i_rep])\n",
    "    \n",
    "    #model = conv2DNet_1(Nchannels, Nsamples_100, Noutputs) #from litterature EEG-Net coorected\n",
    "    model = conv2DNet_2(Noutputs)  #from Temporal - Spatial; 4 Filters Model - Best performing model with accuracy 0.83 in average on the validation set\n",
    "    #model = conv2DNet_3(Noutputs) #from Temporal - Spatial; 64 Filters Model\n",
    "    #model = conv2DNet_4(Noutputs) #from Temporal - Spatial; 128 Filters Model\n",
    "    \n",
    "    #optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.50)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    #optimizer = optim.Adagrad(model.parameters())\n",
    "    #optimizer = optim.Adamax(model.parameters())\n",
    "    #optimizer = optim.ASGD(model.parameters())\n",
    "    #optimizer = optim.RMSprop(model.parameters())\n",
    "    #optimizer = optim.Rprop(model.parameters())\n",
    "    \n",
    "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, verbose=True) #Reduces the learning rate if it did not decreased by more than 10^-4 in 10 steps\n",
    "\n",
    "    for i_ep in range(Nepochs):\n",
    "        for b_start in range(0, Ntrain, batch_size):\n",
    "            bsize_eff = batch_size - max(0, b_start+batch_size-Ntrain)  # boundary case\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            output = model(train_input.narrow(0, b_start, bsize_eff))\n",
    "            if isinstance(criterion, nn.CrossEntropyLoss) or isinstance(criterion, nn.NLLLoss):\n",
    "                batch_loss = criterion(output, train_target.narrow(0, b_start, bsize_eff))\n",
    "            else:\n",
    "                batch_loss = criterion(output.view(bsize_eff*Noutputs), train_target.narrow(0, b_start, bsize_eff))\n",
    "            ep_loss[i_rep, i_ep] += batch_loss.data[0]\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        #scheduler.step(ep_loss[i_rep, i_ep])\n",
    "        \n",
    "        nb_train_errs = compute_nb_errors_delta(model, train_input, train_target, batch_size, criterion)\n",
    "        nb_validation_errs = compute_nb_errors_delta(model, validation_input, validation_target, batch_size, criterion)\n",
    "        \n",
    "        print(\"Epoch Number : \", i_ep)\n",
    "        print(\"\\t Training accuracy: \", (100*(Ntrain*Noutputs-nb_train_errs)/(Ntrain*Noutputs)))\n",
    "        print(\"\\t Validation accuracy \",(100*(Nvalidation*Noutputs-nb_validation_errs)/(Nvalidation*Noutputs)))\n",
    "        \n",
    "        print(\"\\t Epoch Loss \", ep_loss[i_rep, i_ep])\n",
    "        \n",
    "        train_errors[i_rep, i_ep] = nb_train_errs\n",
    "        validation_errors[i_rep, i_ep] = nb_validation_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK SUMMARY & RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = 100*(Ntrain-np.array(train_errors))/Ntrain\n",
    "val_accuracy = 100*(Nvalidation-np.array(validation_errors))/Nvalidation\n",
    "np.save('training_accuracy_Adamax', train_accuracy)\n",
    "np.save('validation_accuracy_Adamax', val_accuracy)\n",
    "\n",
    "stddev_train_errors = np.std(train_accuracy, axis=0)\n",
    "stddev_val_errors = np.std(val_accuracy, axis=0)\n",
    "\n",
    "mean_train_errors = np.mean(train_accuracy, axis=0)\n",
    "mean_val_errors = np.mean(val_accuracy, axis=0)\n",
    "\n",
    "epoch = list(range(50))\n",
    "plt.plot(epoch, mean_train_errors)\n",
    "plt.plot(epoch, mean_val_errors)\n",
    "plt.fill_between(epoch, mean_train_errors+stddev_train_errors, mean_train_errors-stddev_train_errors, alpha=0.5)\n",
    "plt.fill_between(epoch, mean_val_errors+stddev_val_errors, mean_val_errors-stddev_val_errors, alpha=0.5)\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Accuracy in %')\n",
    "plt.legend(['train', 'validation', 'test'])\n",
    "\n",
    "print(\"Training accuracy {:4.3g}%+-{}\".format(mean_train_errors[-1], stddev_train_errors[-1]))\n",
    "print(\"Validation accuracy {:4.3g}%+-{}\".format(mean_val_errors[-1], stddev_val_errors[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target.narrow(0, b_start, bsize_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
